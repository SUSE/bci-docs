[{"id":0,"href":"/bci-docs/guides/adding-users/","title":"Adding Users to SLE BCI Micro and Minimal","parent":"Guides","content":" Note This guide will demonstrate how to add users to the SLE BCI Micro and SLE BCI Minimal images, without having the useradd binary installed. Background The SLE BCI Micro and Minimal images are tailored towards providing a small footprint and thus do not ship the useradd binary. While this reduces the image size, creating new users inside containers based on BCI Micro or Minimal involves a few additional steps.\nSwitch to using the BusyBox SLE BCI SLE BCI Minimal and SLE BCI Micro are lightweight deployment images without a package manager and tailored for specific use cases. If you do not require a package manager in your final image and additionally:\nyou do not need rpm\nyour application runs with POSIX sh and not just Bash\nThen consider using the SLE BCI BusyBox image instead. It is even smaller than SLE BCI Micro and ships the BusyBox implementation of useradd. Adding a new user in BusyBox is straightforward:\nFROM registry.suse.com/bci/bci-busybox:15.7 ARG user # add -H if /home/$user shall not be created RUN adduser -D $user USER $user This container can be built using your favorite container runtime as follows: Docker docker build --build-arg user=rancher . Podman buildah bud --layers --build-arg user=rancher . nerdctl nerdctl build --build-arg user=rancher . Using the Base Container to create the User Account We can utilize a multistage build to create the user in a container that provides the useradd binary and then copy all necessary files into SLE BCI Micro or SLE BCI Minimal. This is achieved using the following Dockerfile:\nFROM registry.suse.com/bci/bci-base:15.7 as useradder ARG user # omit -m if you don\u0026#39;t want /home/$user to be created RUN useradd -m $user FROM registry.suse.com/bci/bci-micro:15.7 ARG user COPY --from=useradder /etc/passwd /etc/passwd COPY --from=useradder /etc/group /etc/group COPY --from=useradder /etc/shadow /etc/shadow # subgid \u0026amp; subuid are rarely necessary in containers # COPY --from=useradder /etc/subgid /etc/subgid # COPY --from=useradder /etc/subuid /etc/subuid # some applications will send your user emails, in case yours does that, # uncomment the following line # COPY --from=useradder /var/spool/mail/$user /var/spool/mail/$user # only include this if you kept the -m flag to useradd COPY --from=useradder /home/$user /home/$user USER $user # remaining build / copy instructions go here Build your container image using your favorite container runtime using the --build-arg parameter as in Switch to using the BusyBox SLE BCI.\nUsing BusyBox to create the User Account We can leverage the adduser implementation from BusyBox to create new users in SLE BCI Minimal, by installing BusyBox inside the Minimal image and then executing its adduser. This will not work in the Micro image as it lacks rpm to install BusyBox.\nWarning This approach will leave two rpm files inside a layer of your final container image, thereby making it slightly bigger than necessary. Consider squashing the layers to remove this overhead. We utilize the SLE BCI Base container once again to download the rpms of BusyBox and libsepol1 (a dependency of BusyBox), copy both rpms into the Minimal image, add the user and remove both packages afterwards:\nFROM registry.suse.com/bci/bci-base:15.7 as downloader RUN zypper download busybox libsepol1 FROM registry.suse.com/bci/bci-minimal:15.7 ARG user ARG arch=x86_64 COPY --from=downloader /var/cache/zypp/packages/SLE_BCI/$arch/*rpm /tmp/ RUN rpm -i /tmp/libsepol1*rpm \u0026amp;\u0026amp; rpm -i /tmp/busybox*rpm \u0026amp;\u0026amp; \\ busybox adduser -D $user \u0026amp;\u0026amp; \\ rpm -e busybox \u0026amp;\u0026amp; rpm -e libsepol1 \u0026amp;\u0026amp; rm -rf /tmp/*rpm USER $user Building this container image requires the additional build argument arch when building on non-x86_64 systems. We also squash the layers, if supported by the container runtime. Currently nerdctl does not support squashing and Docker requires to be launched with experimental features enabled.\nDocker docker build --build-arg user=rancher \\ --build-arg arch=$(uname -m) \\ --squash . Podman buildah bud --build-arg user=rancher \\ --build-arg arch=$(uname -m) \\ --squash . nerdctl nerdctl build --build-arg user=rancher \\ --build-arg arch=$(uname -m) . ","description":" Note This guide will demonstrate how to add users to the SLE BCI Micro and SLE BCI Minimal images, without having the useradd binary installed. Background The SLE BCI Micro and Minimal images are tailored towards providing a small footprint and thus do not ship the useradd binary. While this reduces the image size, creating new users inside containers based on BCI Micro or Minimal involves a few additional steps.\n"},{"id":1,"href":"/bci-docs/documentation/application-stack-bci/","title":"Application Stack Images","parent":"Documentation","content":" The application stack container images offer production-ready applications. These container images don’t include a package manager or the free BCI repository.\nA complete list of images is available here.\n389 DS Image repository: registry.suse.com/suse/389-ds\nThe 389 DS image provides 389 Directory Server, a highly usable, fully featured, reliable and secure LDAP server implementation.\nBIND Image repository: registry.suse.com/suse/bind\nThe BIND image provides BIND 9, a suite of software for interacting with the Domain Name System (DNS).\nCosign Image repository: registry.suse.com/suse/cosign\nThe Cosign image provides Cosign, a tool for signing/verifying OCI containers (and other artifacts) using Sigstore.\nGit Image repository: registry.suse.com/suse/git\nThe Git image provides Git, a distributed version control system that tracks versions of files.\nHelm Image repository: registry.suse.com/suse/helm\nThe Helm image provides Helm, a tool for managing Kubernetes charts, which are packages of pre-configured Kubernetes resources.\nKea Image repository: registry.suse.com/suse/kea\nThe Kea image provides Kea DHCP, an open source DHCP server developed by the Internet Systems Consortium and the successor of the now-deprecated DHCPd.\nKubernetes CLI Image repository: registry.suse.com/suse/kubectl\nThe Kubectl image provides Kubectl (Kubernetes CLI), a command-line tool for communicating with a Kubernetes cluster’s control plane, using the Kubernetes API.\nMariaDB Image repository:\nServer: registry.suse.com/suse/mariadb\nClient: registry.suse.com/suse/mariadb-client\nThe MariaDB images provide MariaDB (Client and Server), an open source, multi-threaded, relational database management system. It’s a backward-compatible branch of the MySQL Community Server that provides a drop-in replacement for MySQL.\nNGINX Image repository: registry.suse.com/suse/nginx\nThe NGINX image provides nginx, an open source reverse proxy server for the HTTP, HTTPS, SMTP, POP3, and IMAP protocols. nginx can also act as a load balancer, HTTP cache, and a web server (origin server).\nPerformance Co-Pilot Image repository: registry.suse.com/suse/pcp\nThe Performance Co-Pilot image provides Performance Co-Pilot (PCP), a system performance analysis toolkit.\nPostgreSQL Image repository: registry.suse.com/suse/postgres\nThe PostgreSQL image provides PostgreSQL, an extensible and SQL-compliant relational database management system (RDBMS).\nRegistry Image repository: registry.suse.com/suse/registry\nThe Registry image provides Distribution Registry, a stateless, highly scalable application that stores and lets you distribute OCI container images and other content.\nRMT Image repository: registry.suse.com/suse/rmt-server\nThe RMT image provides a containerized version of the SUSE Repository Mirroring Tool (RMT) server for Kubernetes deployments.\nStunnel Image repository: registry.suse.com/suse/stunnel\nThe Stunnel image provides Stunnel, an open source multi-platform application that provides a universal TLS/SSL tunneling service.\nValkey Image repository: registry.suse.com/suse/valkey\nThe Valkey image provides Valkey, a high-performance data structure server that primarily serves key/value workloads.\n","description":" The application stack container images offer production-ready applications. These container images don’t include a package manager or the free BCI repository.\nA complete list of images is available here.\n389 DS Image repository: registry.suse.com/suse/389-ds\nThe 389 DS image provides 389 Directory Server, a highly usable, fully featured, reliable and secure LDAP server implementation.\nBIND Image repository: registry.suse.com/suse/bind\nThe BIND image provides BIND 9, a suite of software for interacting with the Domain Name System (DNS).\n"},{"id":2,"href":"/bci-docs/guides/use-with-golang/","title":"Building and Deploying Go Applications","parent":"Guides","content":" There is a SLE BCI that can be used with the Go programming language. There are a couple different recommended methods to work with the Go SLE BCI.\nDon’t Ship The Compiler Go is a compiled language producing a binary as the end result. That means the compiler does not need to be shipped as part of the images that are distributed. Instead, it is recommended that the Go image is used as the builder image only.\nBy not shipping the Go compiler with your application, the attack surface area of the containerized application is reduced and the overall image size is much smaller.\nUsing Go As A Builder Image There are two ways to work with Go images. First, you can encapsulate your application in a scratch container image, which is essentially an empty image. This approach will not function if your Go application depends on libc or any other library, as the libraries will not be available.\nA second method is to use a slim base container image with just the minimal packages needed. The General Purpose SLE BCI images offer four different options here, depending on your exact requirements.\nBuilding from scratch The following Dockerfile illustrates building an application using the SLE BCI Go image to compile the binary and then copying it into a new image based on scratch. The example uses a hello world as the program name, which can be substituted for the real application name.\n# Build the Go Binary using the SLE BCI Go images FROM registry.suse.com/bci/golang:stable as build WORKDIR /app COPY go.mod ./ COPY go.sum ./ RUN go mod download COPY *.go ./ # Make sure to build the application with CGO disabled. This will force Go to # use some Go implementations of code rather than those normally supplied by the # host operating system. You need this for scratch images as those supporting # libraries are not available. RUN CGO_ENABLED=0 go build -o /hello-world # Create image to bundle app FROM scratch COPY --from=build /hello-world /hello-world CMD [\u0026#34;/hello-world\u0026#34;] Additional settings like exposing network ports or running as a non-root user can be specified in the last step below the FROM scratch line.\nBuilding from SLE BCI Applications that require external libraries or CA certificates cannot be deployed into a scratch image. A General Purpose SLE BCI should be used instead. The above Dockerfile has to be slightly adjusted in this case:\n# Build the Go Binary using the SLE BCI Go images FROM registry.suse.com/bci/golang:stable as build WORKDIR /app COPY go.mod ./ COPY go.sum ./ RUN go mod download COPY *.go ./ RUN go build -o /hello-world # Create image to bundle app FROM registry.suse.com/bci/bci-micro:latest # Install dependencies (if required) here COPY --from=build /hello-world /usr/local/bin/hello-world CMD [\u0026#34;/usr/local/bin/hello-world\u0026#34;] The above example uses the SLE BCI micro image as the deployment image for the resulting application. This is just one of the options, other options can be found in the section about the General Purpose SLE BCI.\n","description":" There is a SLE BCI that can be used with the Go programming language. There are a couple different recommended methods to work with the Go SLE BCI.\nDon’t Ship The Compiler Go is a compiled language producing a binary as the end result. That means the compiler does not need to be shipped as part of the images that are distributed. Instead, it is recommended that the Go image is used as the builder image only.\n"},{"id":3,"href":"/bci-docs/guides/building-on-top-of-bci/","title":"Building Container Images Based on SLE BCI","parent":"Guides","content":" In the container ecosystem, many tools can work with OCI-compliant images, and all of them can use our Base Container Images.\nUsing with Docker, Podman or nerdctl The Base Container Images are OCI-compliant, and you can use them directly in your Dockerfile or Containerfile without any modifications. All you need to do is include the image in the FROM line as follows:\nFROM registry.suse.com/bci/nodejs:latest as node-builder WORKDIR /app/ COPY . /app/ RUN npm install \u0026amp;\u0026amp; npm run build Build it with your favorite container runtime: Docker docker build -t my-app . Podman podman build -t my-app . nerdctl nerdctl build -t my-app . Using the Open Build Service The Open Build Service (OBS) lets you build container images, as explained in the documentation. It supports Docker, Podman and KIWI as back-ends.\nThe examples below require a project where you have write access and osc. Your home project (home:\u0026lt;your-username\u0026gt;) is sufficient for this exercise.\nBuilding a container using Docker or Podman To build a Dockerfile-based container image in OBS, follow the steps below.\nCreate a new package for your container.\nosc checkout home:your-username \u0026amp;\u0026amp; cd home:your-username osc mkpac my-container-image \u0026amp;\u0026amp; cd my-container-image Add a repository to the project with osc. The repository name is containerfile by convention. This allows OBS to fetch resources used to build containers.\nosc meta -e prj Insert the following XML into the project settings:\n\u0026lt;repository name=\u0026#34;containerfile\u0026#34;\u0026gt; \u0026lt;path project=\u0026#34;SUSE:Registry\u0026#34; repository=\u0026#34;standard\u0026#34;/\u0026gt; \u0026lt;path project=\u0026#34;SUSE:SLE-15-SP7:Update\u0026#34; repository=\u0026#34;standard\u0026#34;/\u0026gt; \u0026lt;arch\u0026gt;x86_64\u0026lt;/arch\u0026gt; \u0026lt;arch\u0026gt;aarch64\u0026lt;/arch\u0026gt; \u0026lt;arch\u0026gt;s390x\u0026lt;/arch\u0026gt; \u0026lt;arch\u0026gt;ppc64le\u0026lt;/arch\u0026gt; \u0026lt;/repository\u0026gt; The path SUSE:Registry allows the use of images from the SUSE Registry in OBS. The path SUSE:SLE-15-SP7:Update allows the container to use packages from SLE. Depending on the SLE version that you are targeting in your container, this path needs adjustment.\nAdd a configuration to the project with osc. This will configure OBS to use Docker or Podman to build packages in the containerfile repository.\nosc meta -e prjconf Insert the following into the project configuration:\n%if %_repository == \u0026#34;containerfile\u0026#34; # OBS supports the following engines as backend: # - podman # - docker Type: podman %endif Create a Dockerfile inside the package my-container-image. Set the base image using FROM, as usual.\nFROM registry.suse.com/bci/bci-base:15.7 Set the build tags using comments in the Dockerfile.\n#!BuildTag: my-build-tag:latest #!BuildTag: my-build-tag:0.1 #!BuildTag: my-other-build-tag:0.1 The BuildTag is the equivalent image name (or tag) assigned during the build process. Since OBS invokes Docker or Podman itself, it has the same effect as the following command:\npodman build -t my-build-tag:latest -t my-build-tag:0.1 -t my-other-build-tag:0.1 . A complete Dockerfile would look like this:\n#!BuildTag: my-app:latest #!BuildTag: my-app:0.0.1 FROM registry.suse.com/bci/bci-base:15.7 WORKDIR /src RUN zypper -n install --no-recommends make gcc COPY . . RUN make CMD [\u0026#34;./my-app\u0026#34;] Building a container using KIWI KIWI is a generic image-building tool that also supports building container images. It is tightly integrated into the Open Build Service as the standard image builder.\nTo build a KIWI-based container image in OBS, follow the steps below.\nCreate a new package for your container.\nosc checkout home:your-username \u0026amp;\u0026amp; cd home:your-username osc mkpac my-kiwi-container \u0026amp;\u0026amp; cd my-kiwi-container Add a repository to the project with osc. The repository name is containerkiwi by convention. This allows OBS to fetch resources used to build containers.\nosc meta -e prj Insert the following XML into the project settings:\n\u0026lt;repository name=\u0026#34;containerkiwi\u0026#34;\u0026gt; \u0026lt;path project=\u0026#34;SUSE:Registry\u0026#34; repository=\u0026#34;standard\u0026#34;/\u0026gt; \u0026lt;path project=\u0026#34;SUSE:SLE-15-SP7:Update\u0026#34; repository=\u0026#34;standard\u0026#34;/\u0026gt; \u0026lt;arch\u0026gt;x86_64\u0026lt;/arch\u0026gt; \u0026lt;arch\u0026gt;aarch64\u0026lt;/arch\u0026gt; \u0026lt;arch\u0026gt;s390x\u0026lt;/arch\u0026gt; \u0026lt;arch\u0026gt;ppc64le\u0026lt;/arch\u0026gt; \u0026lt;/repository\u0026gt; Add a configuration to the project with osc. This will configure OBS to KIWI to build packages in the containerkiwi repository.\nosc meta -e prjconf Insert the following into the project configuration:\n%if \u0026#34;%_repository\u0026#34; == \u0026#34;containerkiwi\u0026#34; Type: kiwi Repotype: none Patterntype: none Prefer: -libcurl4-mini Prefer: -systemd-mini Prefer: -libsystemd0-mini Prefer: -libudev-mini1 Prefer: -udev-mini Prefer: kiwi-boot-requires Prefer: sles-release Prefer: sles-release-MINI Prefer: python3-kiwi Preinstall: !rpm rpm-ndb Substitute: rpm rpm-ndb Binarytype: rpm %endif Create a kiwi.xml inside the package my-kiwi-image.\n\u0026lt;image schemaversion=\u0026#34;7.4\u0026#34; name=\u0026#34;my-kiwi-image\u0026#34;\u0026gt; \u0026lt;description type=\u0026#34;system\u0026#34;\u0026gt; \u0026lt;!-- omitted --\u0026gt; \u0026lt;/description\u0026gt; \u0026lt;preferences\u0026gt; \u0026lt;!-- Refer to SLE BCI images by using `obsrepositories` --\u0026gt; \u0026lt;type image=\u0026#34;docker\u0026#34; derived_from=\u0026#34;obsrepositories:/bci/bci-base#15.7\u0026#34;\u0026gt; \u0026lt;containerconfig name=\u0026#34;my-kiwi-image\u0026#34; tag=\u0026#34;0.0.1\u0026#34; additionaltags=\u0026#34;latest\u0026#34;\u0026gt; \u0026lt;labels\u0026gt; \u0026lt;!-- add your labels here --\u0026gt; \u0026lt;label name=\u0026#34;org.opencontainers.image.title\u0026#34; value=\u0026#34;My KIWI Image\u0026#34;/\u0026gt; \u0026lt;/labels\u0026gt; \u0026lt;subcommand execute=\u0026#34;/bin/sh\u0026#34;/\u0026gt; \u0026lt;/containerconfig\u0026gt; \u0026lt;/type\u0026gt; \u0026lt;version\u0026gt;15.7.0\u0026lt;/version\u0026gt; \u0026lt;packagemanager\u0026gt;zypper\u0026lt;/packagemanager\u0026gt; \u0026lt;rpm-excludedocs\u0026gt;true\u0026lt;/rpm-excludedocs\u0026gt; \u0026lt;/preferences\u0026gt; \u0026lt;repository type=\u0026#34;rpm-md\u0026#34;\u0026gt; \u0026lt;source path=\u0026#34;obsrepositories:/\u0026#34;/\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;packages\u0026gt; \u0026lt;!-- add your packages here --\u0026gt; \u0026lt;package name=\u0026#34;gcc\u0026#34;/\u0026gt; \u0026lt;package name=\u0026#34;make\u0026#34;/\u0026gt; \u0026lt;/packages\u0026gt; \u0026lt;/image\u0026gt; Building images based on your images You can build containers in OBS that are based on other containers that have been built in OBS as well.\nIf the image you want to use is in the same project and repository as the image that you are building, there’s no need to configure any extra repositories.\nHowever, if the image comes from another project or repository, you need to adjust your repository configuration. Add the desired repository path to the project with osc. Previously, we added the repositories containerfile and containerkiwi to use the SLE BCI images. Now we are going to include another project path.\nosc meta -e prj Adjust the containerfile or containerkiwi XML to include a new path:\n\u0026lt;repository name=\u0026#34;containerfile\u0026#34;\u0026gt; \u0026lt;path project=\u0026#34;SUSE:Registry\u0026#34; repository=\u0026#34;standard\u0026#34;/\u0026gt; \u0026lt;path project=\u0026#34;SUSE:SLE-15-SP7:Update\u0026#34; repository=\u0026#34;standard\u0026#34;/\u0026gt; \u0026lt;path project=\u0026#34;PROJECT:NAME\u0026#34; repository=\u0026#34;repository-name\u0026#34;/\u0026gt; \u0026lt;arch\u0026gt;x86_64\u0026lt;/arch\u0026gt; \u0026lt;arch\u0026gt;aarch64\u0026lt;/arch\u0026gt; \u0026lt;arch\u0026gt;s390x\u0026lt;/arch\u0026gt; \u0026lt;arch\u0026gt;ppc64le\u0026lt;/arch\u0026gt; \u0026lt;/repository\u0026gt; As shown in the previous sections, now you can use other images similarly to SLE BCI.\n","description":" In the container ecosystem, many tools can work with OCI-compliant images, and all of them can use our Base Container Images.\nUsing with Docker, Podman or nerdctl The Base Container Images are OCI-compliant, and you can use them directly in your Dockerfile or Containerfile without any modifications. All you need to do is include the image in the FROM line as follows:\nFROM registry.suse.com/bci/nodejs:latest as node-builder WORKDIR /app/ COPY . /app/ RUN npm install \u0026amp;\u0026amp; npm run build "},{"id":4,"href":"/bci-docs/categories/","title":"Categories","parent":"Introduction to SUSE Linux Base Container Images","content":"","description":""},{"id":5,"href":"/bci-docs/guides/deploy-using-zypper/","title":"Deploy an Application using zypper","parent":"Guides","content":" Scope The purpose of this guide is to deploy an application or the dependencies of an application from rpms into a deployment image using the zypper package manager.\nUsing zypper’s custom root Zypper provides the --installroot flag to install packages into a custom root and not use /. We can leverage this to install packages including all of their dependencies into a custom root and then copy this directory into a deployment image. In the following example we install apache2 including all of its dependencies and copy them into the deployment image based on bci-micro:\nFROM registry.suse.com/bci/bci-micro:latest AS micro FROM registry.suse.com/bci/bci-base:latest AS builder COPY --from=micro / /chroot/ RUN zypper --installroot /chroot -n --gpg-auto-import-keys in --no-recommends apache2 \u0026amp;\u0026amp; \\ zypper --installroot /chroot clean -a \u0026amp;\u0026amp; \\ rm -rf /chroot/var/log/ FROM micro WORKDIR / COPY --from=builder /chroot/ / Customizing zypper’s installation behavior We can further reduce the final image size by supplying a custom zypper configuration file. We can tweak zypper’s behavior further via that configuration file to e.g. omit the installation of documentation files. To achieve this omission of documentation files, create the following scratch-zypp.conf:\n[main] rpm.install.excludedocs = yes And modify the Dockerfile as follows:\nFROM registry.suse.com/bci/bci-micro:latest AS micro FROM registry.suse.com/bci/bci-base:latest AS builder COPY --from=micro / /chroot/ COPY scratch-zypp.conf / ENV ZYPP_CONF=/scratch-zypp.conf RUN zypper --installroot /chroot -n --gpg-auto-import-keys in --no-recommends apache2 \u0026amp;\u0026amp; \\ zypper --installroot /chroot clean -a \u0026amp;\u0026amp; \\ rm -rf /chroot/var/log/ FROM micro WORKDIR / COPY --from=builder /chroot/ / ","description":" Scope The purpose of this guide is to deploy an application or the dependencies of an application from rpms into a deployment image using the zypper package manager.\nUsing zypper’s custom root Zypper provides the --installroot flag to install packages into a custom root and not use /. We can leverage this to install packages including all of their dependencies into a custom root and then copy this directory into a deployment image. In the following example we install apache2 including all of its dependencies and copy them into the deployment image based on bci-micro:\n"},{"id":6,"href":"/bci-docs/documentation/","title":"Documentation","parent":"Introduction to SUSE Linux Base Container Images","content":" Introduction Getting Started Images General Purpose Language Stack Application Stack Frequently Asked Questions ","description":" Introduction Getting Started Images General Purpose Language Stack Application Stack Frequently Asked Questions "},{"id":7,"href":"/bci-docs/documentation/faq/","title":"Frequently Asked Questions","parent":"Documentation","content":" What are the SUSE Linux Base Container Images? SUSE Linux Base Container Images (SUSE Linux BCI) provides a set of container images that are truly open, flexible and secure. Available for immediate use by developers, integrators and operators are container images based on SUSE Linux Enterprise Server (SLES). SUSE Linux BCI includes general-purpose container images, development tools and applications.\nWhat are the benefits of SUSE Linux BCI? Free to Use \u0026amp; Redistribute\nSUSE Linux BCI containers are completely free and come with an EULA that enables you to redistribute them freely.\nNo subscription required, ideal for development, CI/CD and production without subscription hurdles.\nEnterprise-Grade Security\nBuilt from the same code base as SUSE Linux Enterprise Server.\nMaintained with enterprise-grade CVE mitigation and aligned with SLES’s certified, proactive security.\nSLSA L3 grade builds.\nSigned container images for supply chain integrity.\nSBOM available in both SPDX and CycloneDX formats.\nSLSA Provenance file available.\nStability\nEnterprise lifecycle, aligned with the lifecycle of the corresponding SUSE Linux Enterprise Server version.\nConsistent ABI/API.\nVendor Neutral\nWorks on any OCI-compliant runtime.\nWorks on any Linux OS.\nNo vendor lock-in.\nMulti-architecture\nAvailable for up to 4 architectures[1]: amd64, arm64, ppc64le and s390x.\nCompliance \u0026amp; Certification\nEvaluation Assurance Level 4+ (Common Criteria) inherited from SUSE Linux Enterprise Server. SLES 15 is the only general-purpose OS with an active EAL 4+ certificate. See SUSE Certifications and Features for more details.\nSelected containers can be configured in FIPS mode.\nFIPS certification[2] inherited from SUSE Linux Enterprise Server for selected containers.\nSmooth transition to Enterprise Support\nStart free and when you need commercial support, subscribe and stay on the same container images.\nNo need to rebuild your stack when moving from community to enterprise.\nIs SUSE Linux BCI free to use? Yes. SUSE Linux BCI is free to use and redistribute in accordance with the SUSE Linux BCI EULA.\nWhere can I find SUSE Linux BCI? The container images are available on the SUSE Container Registry.\nWhat is included in SUSE Linux BCI? Leveraging SUSE’s industry-leading build pipeline, SUSE Linux BCI provides a set of container images where all binaries are built and maintained by SUSE, drawing directly from the trusted SLES codebase.\nBase containers come in several flavors. Choose the BCI Micro image for a minimal footprint or the BCI Base image with full package-management tools. Expand the container images easily using thousands of packages from the free BCI repository.\nLanguage containers are available as a base environment for development and deployment. Use programming languages such as C++, Go, Java (OpenJDK), .NET[3], Node.js, PHP, Python, Ruby and Rust.\nApplication containers provide ready-to-use databases, tools and more. Use applications such as 389 Directory Server, Cosign, Git, Helm, MariaDB, Nginx, Performance Co-Pilot, PostgreSQL, RMT and Valkey.\nWhat is the difference between SUSE Linux BCI Base, Minimal, Micro and BusyBox? The SUSE Linux BCI Base container has Zypper and access to the free BCI repository. This allows you to install any packages from the available repositories.\nThe SUSE Linux BCI Minimal container has RPM but does not include Zypper. It is an image designed for deployment scenarios where you copy the final artifacts to it.\nThe SUSE Linux BCI Micro container does not include Zypper and RPM. It is similar to BCI Minimal but designed for the deployment of static binaries.\nThe SUSE Linux BCI BusyBox container has no GPLv3-licensed software, Zypper, or RPM. It is comparable to BCI Micro, but it replaces the GNU Coreutils with BusyBox tools.\nWhat is the free BCI repository? The free BCI repository is a repository with packages that are free to use and redistribute, subject to the terms of the SUSE Linux BCI EULA.\nAll container images with Zypper include the free BCI repositories pre-configured.\nWhat packages and libraries can I use to expand SUSE Linux BCI? The container images that include Zypper have access to the free BCI repository. This repository contains thousands of SUSE-supported packages. SUSE will support these packages when the container is running on a SUSE host with the appropriate subscription.\nYou are also free to configure repositories from openSUSE Leap or other compatible sources.\nWhat are some use cases for SUSE Linux BCI? SUSE Linux BCI provides a stable, secure and open ecosystem for developing and deploying applications. SUSE Linux BCI leverages the experience, stability and security of SUSE Linux Enterprise Server.\nSUSE Linux BCI is suitable for different use cases:\nSUSE Rancher\nEnable Rancher to build using stable, reliable, secure and certified enterprise components.\nLeverage SUSE’s in-house OS knowledge while containerizing applications.\nEasy migration path from OS-based applications to containerized applications.\nDevelopers\nAvoid vendor lock-in.\nDeploy to any Linux host.\nFreely redistributable.\nSuitable for development, testing and production environments.\nIndependent Software Vendors (ISVs)\nContainerize applications using a stable, reliable, secure and certified enterprise OS.\nLeverage SUSE security and supply chain.\nRun applications on various hosts.\nWhy did SUSE create SUSE Linux BCI? SUSE created SUSE Linux BCI to provide truly open, flexible and secure container images that developers and integrators can use without the vendor lock-in of alternative offerings.\nOn which hardware platforms is SUSE Linux BCI available? SUSE Linux BCI is available on amd64 (x86_64), arm64 (aarch64), ppc64le and s390x. However, architecture availability may vary depending on the specific image.\nDo I need a subscription to use SUSE Linux BCI? No, you can use SUSE Linux BCI for free without a subscription. Just pull the images from the SUSE Container Registry and start using them.\nSUSE Linux BCI under an active SUSE Linux Enterprise Server subscription provides full support and access to SLES repositories.\nDo I need a SUSE host to build BCI-based images? No, you can build and run SUSE Linux BCI in any Linux environment that supports OCI-compatible images.\nDo I need a SUSE host to deploy SUSE Linux BCI? No, you can deploy SUSE Linux BCI to any OCI-compatible runtime or certified Kubernetes deployment.\nCan I distribute BCI-based images? Yes, BCI-based images can be freely distributed as long as you follow the SUSE Linux BCI EULA.\nAll packages in the free BCI repository are freely redistributable. If you use only this repository, your container remains redistributable.\nIf you add more content to your container, you may need to check for additional restrictions.\nCan I distribute BCI-based images on any registry? Yes, you can distribute your BCI-based image in any form or registry you want.\nCan I add third-party software to BCI-based images and still redistribute them? Yes, but adding third-party software or repositories implies restrictions at their respective layers.\nSUSE places no restrictions on redistributing the results, as long as you follow the SUSE Linux BCI EULA.\nDoes SUSE Linux BCI receive regular package updates? Yes, container images receive frequent security updates and version upgrades. It follows the same principles and release model as SUSE Linux Enterprise Server.\nDoes SUSE Linux BCI receive regular security updates? Yes, container images are rebuilt regularly and updated with SUSE’s enterprise-grade security updates.\nDoes SUSE Linux BCI have FIPS mode images? Yes, FIPS mode container images are available.\nBCI Base FIPS configures FIPS mode enabled by default. However, it does not include any certified binaries.\nBCI Go includes a FIPS 140-3 enabled Go.\nCan I use SUSE Linux BCI for community projects? Yes, as long as you follow the SUSE Linux BCI EULA.\nWhat is the lifecycle of SUSE Linux BCI? The SLES-based container images follow the same support lifecycle as their SLES release.\nApplication and Language images follow the lifecycle of their respective tools rather than the SLES release. For further details, check the SUSE Product Support Lifecycle.\nLong Term Service Pack (LTSS) versions of SUSE Linux BCI are available to customers with an LTSS subscription.\nHow is SUSE Linux BCI supported? As a free-to-use product under the SUSE Linux BCI EULA, SUSE Linux BCI receives community support.\nWhen BCI-based containers run on a SUSE host with an active subscription, they receive the same support as the host. This also applies to any packages installed from the free BCI repository.\nYou can install any package from the SLES repositories covered by your subscription and receive the same support. However, packages from the SLES repositories are not redistributable.\nWhere do I report bugs with SUSE Linux BCI? Customers and partners with active subscriptions can use the regular channels to report issues and request support.\nFor community support, report bugs in Bugzilla under PUBLIC SUSE Linux Base Container Images.\nWhere do I report issues with SUSE Linux BCI documentation? Report issues with documentation, guides and examples in Bugzilla under PUBLIC SUSE Linux Base Container Images.\nCan I request packages and images to be added to SUSE Linux BCI? Yes, you can request new features, but these require evaluation.\nCustomers and partners with active subscriptions can use the regular channels to request new features.\nFor community support, report bugs in Bugzilla under PUBLIC SUSE Linux Base Container Images.\n1. Some containers may be available in a smaller number of architectures. 2. Containers out of the General Availability phase are available to customers with corresponding LTS SUSE Linux Enterprise Server subscriptions. 3. Available in Tech Preview, .NET binaries provided and maintained by Microsoft. ","description":" What are the SUSE Linux Base Container Images? SUSE Linux Base Container Images (SUSE Linux BCI) provides a set of container images that are truly open, flexible and secure. Available for immediate use by developers, integrators and operators are container images based on SUSE Linux Enterprise Server (SLES). SUSE Linux BCI includes general-purpose container images, development tools and applications.\nWhat are the benefits of SUSE Linux BCI? Free to Use \u0026amp; Redistribute\nSUSE Linux BCI containers are completely free and come with an EULA that enables you to redistribute them freely.\nNo subscription required, ideal for development, CI/CD and production without subscription hurdles.\n"},{"id":8,"href":"/bci-docs/documentation/general-purpose-bci/","title":"General Purpose Images","parent":"Documentation","content":" The general-purpose container images include a minimum set of packages to keep their size small. You can use it as a starting point for building other container images or for deploying specific software using the free BCI repository.\nA complete list of images is available here.\nBase—When you need flexibility Image repository: registry.suse.com/bci/bci-base\nThe Base image comes with a package manager (Zypper and RPM) and the free SUSE Linux BCI repository pre-configured, allowing you to install packages from the repository and customize the image.\nInit—When you need a Systemd environment Image repository: registry.suse.com/bci/bci-init\nThe Init image adds Systemd on top of the Base image. This container is only supported with Podman.\nMinimal—When you do not need Zypper Image repository: registry.suse.com/bci/bci-minimal\nThe Minimal image is a stripped-down version of Base. It comes with the RPM package manager but not Zypper, reducing its size. However, while RPM can install and remove packages, it lacks support for repositories and automated dependency resolution.\nMicro—When you need a small image Image repository: registry.suse.com/bci/bci-micro\nThe Micro image has fewer preinstalled packages and no package managers. It is ideal for deploying binaries compiled externally or in a previous build stage.\nWe recommend using Micro as the final stage in a multi-stage build when deploying a project.\nBusyBox—When you need the smallest and GPLv3-free image Image repository: registry.suse.com/bci/bci-busybox\nThe BusyBox image includes tools provided by BusyBox. This image is smaller than Micro and contains no GPLv3-licensed software.\nWhen using BusyBox tools, keep in mind that they differ from GNU Coreutils. Scripts written for Bash or coreutils may require modification.\nFIPS—When you need validated, encrypted modules Image repository:\nBase: registry.suse.com/bci/bci-base-fips\nMicro: registry.suse.com/bci/bci-micro-fips\nThe Base and Micro images are also offered with FIPS mode enabled. However, it does not include any certified binaries. FIPS images with certified binaries are available through an LTSS subscription.\nUse these images in deployment scenarios that require FIPS 140-2 or FIPS 140-3.\nApproximate image size Below are the approximate sizes of each image.\nSUSE Linux BCI Base ~100-120 MB\nSUSE Linux BCI Base FIPS ~150-175 MB\nSUSE Linux BCI Init ~140-150 MB\nSUSE Linux BCI Minimal ~35-45 MB\nSUSE Linux BCI Micro ~25 MB\nSUSE Linux BCI Micro FIPS ~35 MB\nSUSE Linux BCI BusyBox ~15 MB\n","description":" The general-purpose container images include a minimum set of packages to keep their size small. You can use it as a starting point for building other container images or for deploying specific software using the free BCI repository.\nA complete list of images is available here.\nBase—When you need flexibility Image repository: registry.suse.com/bci/bci-base\nThe Base image comes with a package manager (Zypper and RPM) and the free SUSE Linux BCI repository pre-configured, allowing you to install packages from the repository and customize the image.\n"},{"id":9,"href":"/bci-docs/documentation/getting-started-guide/","title":"Getting Started","parent":"Documentation","content":" SUSE Linux Base Container Images (SUSE Linux BCI) are available from the SUSE Registry. Use it like any other OCI-compatible container image.\nTo use SUSE Linux BCI Base from the command line, simply run from your preferred container runtime:\nDocker $ docker run --rm -it registry.suse.com/bci/bci-base:latest grep \u0026#39;^NAME\u0026#39; /etc/os-release NAME=\u0026#34;SLES\u0026#34; Podman $ podman run --rm -it registry.suse.com/bci/bci-base:latest grep \u0026#39;^NAME\u0026#39; /etc/os-release NAME=\u0026#34;SLES\u0026#34; nerdctl $ nerdctl run --rm -it registry.suse.com/bci/bci-base:latest grep \u0026#39;^NAME\u0026#39; /etc/os-release NAME=\u0026#34;SLES\u0026#34; To use SUSE Linux BCI Base in a Dockerfile, do the following:\nFROM registry.suse.com/bci/bci-base:latest RUN zypper --non-interactive install python3 CMD [\u0026#34;python3\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;print(\u0026#39;Hello SUSE Linux BCI!\u0026#39;)\u0026#34;] Build the container image using your preferred container runtime:\nDocker docker build -t my-app . Podman podman build -t my-app . nerdctl nerdctl build -t my-app . Run the container image you built using your preferred container runtime:\nDocker docker run -it --rm my-app Podman podman run -it --rm my-app nerdctl nerdctl run -it --rm my-app ","description":" SUSE Linux Base Container Images (SUSE Linux BCI) are available from the SUSE Registry. Use it like any other OCI-compatible container image.\nTo use SUSE Linux BCI Base from the command line, simply run from your preferred container runtime:\nDocker $ docker run --rm -it registry.suse.com/bci/bci-base:latest grep \u0026#39;^NAME\u0026#39; /etc/os-release NAME=\u0026#34;SLES\u0026#34; Podman $ podman run --rm -it registry.suse.com/bci/bci-base:latest grep \u0026#39;^NAME\u0026#39; /etc/os-release NAME=\u0026#34;SLES\u0026#34; nerdctl $ nerdctl run --rm -it registry.suse.com/bci/bci-base:latest grep \u0026#39;^NAME\u0026#39; /etc/os-release NAME=\u0026#34;SLES\u0026#34; "},{"id":10,"href":"/bci-docs/guides/","title":"Guides","parent":"Introduction to SUSE Linux Base Container Images","content":" Using SLE BCI Using with VS Code Development Containers Verify SLE BCI Building Container Images Based on SLE BCI Adding Users to BCI Micro and Minimal Building and Deploying Go Applications How to Use container-suseconnect Launch Containers with Podman and Systemd Deploy an Application Using zypper How to Build a Distroless Image Using SLE BCI ","description":" Using SLE BCI Using with VS Code Development Containers Verify SLE BCI Building Container Images Based on SLE BCI Adding Users to BCI Micro and Minimal Building and Deploying Go Applications How to Use container-suseconnect Launch Containers with Podman and Systemd Deploy an Application Using zypper How to Build a Distroless Image Using SLE BCI "},{"id":11,"href":"/bci-docs/guides/building-a-distroless-image/","title":"How to build a distroless image using SLE BCI","parent":"Guides","content":" What is a distroless image? Distroless images are stripped down container images, where the underlying Linux distribution is reduced to the bare minimum. A distroless image normally contains only certificates and specific core libraries, and it does not include a shell or utilities like cat or ls.\nThe advantages of distroless images include smaller size and potentially fewer vulnerabilities.\nThe major disadvantage is the difficulty of debugging a containerized application, as the container image does not provide any debugging tools and may even lack tools to read log files. Debugging applications usually requires attaching sidecar containers or interacting with the container via the /proc/ filesystem. For this reason, we recommend to use SLE BCI Base, SLE BCI Micro or SLE BCI BusyBox as the deployment image, because they make debugging easier, and they come with a valid rpm database.\nWarning A distroless image, or any image without the rpm database, makes it harder for container security scanners to find known vulnerabilities in the container image. Keep in mind that if a scanner relies only on the rpm database, it cannot detect a vulnerable shared library. How to build a distroless images from SLE BCI Warning The approach described in this section is unsupported and can lead to broken image as only shared libraries will be copied and no other required files are copied.\nFor a safer method, refer to Deploy an Application using zypper.\nAlthough SUSE does not offer a distroless SLE BCI, you can build one by creating a multi-stage build and creating a final image based on SCRATCH. The following tutorial uses an existing application to demonstrate how to identify its dependent libraries and copy them into the final image.\nThe first step is to identify all the components required by the application. This can include configuration files, external binaries, and shared libraries. It is your task to determine which configuration files and binaries are required for the program to function correctly. For example, a Python application will require that the Python interpreter is present in the final image.\nAs minimum, compiled applications are usually dynamically linked to libc. It is possible to statically link against most libraries, but not against glibc (which is the libc implementation used by SLE). Therefore the compiled application requires at least the shared libc library. The required libraries can be identified using ldd. The following example deploys the Rust package manager cargo in an empty image.\nRun the ldd /usr/bin/cargo command to obtain all shared libraries against which cargo is linked:\n# ldd /usr/bin/cargo linux-vdso.so.1 (0x00007ffda3f42000) libz.so.1 =\u0026gt; /lib64/libz.so.1 (0x00007f3766c14000) libcurl.so.4 =\u0026gt; /usr/lib64/libcurl.so.4 (0x00007f3767c6a000) libssl.so.1.1 =\u0026gt; /usr/lib64/libssl.so.1.1 (0x00007f3767bcb000) libcrypto.so.1.1 =\u0026gt; /usr/lib64/libcrypto.so.1.1 (0x00007f37668d5000) libgcc_s.so.1 =\u0026gt; /lib64/libgcc_s.so.1 (0x00007f37666b6000) libpthread.so.0 =\u0026gt; /lib64/libpthread.so.0 (0x00007f3766493000) libm.so.6 =\u0026gt; /lib64/libm.so.6 (0x00007f3766148000) libdl.so.2 =\u0026gt; /lib64/libdl.so.2 (0x00007f3765f44000) libc.so.6 =\u0026gt; /lib64/libc.so.6 (0x00007f3765b4f000) /lib64/ld-linux-x86-64.so.2 (0x00007f3767ae5000) libnghttp2.so.14 =\u0026gt; /usr/lib64/libnghttp2.so.14 (0x00007f3765927000) libidn2.so.0 =\u0026gt; /usr/lib64/libidn2.so.0 (0x00007f376570a000) libssh.so.4 =\u0026gt; /usr/lib64/libssh.so.4 (0x00007f376549c000) libpsl.so.5 =\u0026gt; /usr/lib64/libpsl.so.5 (0x00007f376528a000) libgssapi_krb5.so.2 =\u0026gt; /usr/lib64/libgssapi_krb5.so.2 (0x00007f3765038000) libldap_r-2.4.so.2 =\u0026gt; /usr/lib64/libldap_r-2.4.so.2 (0x00007f3764de4000) liblber-2.4.so.2 =\u0026gt; /usr/lib64/liblber-2.4.so.2 (0x00007f3764bd5000) libzstd.so.1 =\u0026gt; /usr/lib64/libzstd.so.1 (0x00007f37648a5000) libbrotlidec.so.1 =\u0026gt; /usr/lib64/libbrotlidec.so.1 (0x00007f3764699000) libjitterentropy.so.3 =\u0026gt; /usr/lib64/libjitterentropy.so.3 (0x00007f3764492000) libunistring.so.2 =\u0026gt; /usr/lib64/libunistring.so.2 (0x00007f376410f000) libkrb5.so.3 =\u0026gt; /usr/lib64/libkrb5.so.3 (0x00007f3763e36000) libk5crypto.so.3 =\u0026gt; /usr/lib64/libk5crypto.so.3 (0x00007f3763c1e000) libcom_err.so.2 =\u0026gt; /lib64/libcom_err.so.2 (0x00007f3763a1a000) libkrb5support.so.0 =\u0026gt; /usr/lib64/libkrb5support.so.0 (0x00007f376380b000) libresolv.so.2 =\u0026gt; /lib64/libresolv.so.2 (0x00007f37635f3000) libsasl2.so.3 =\u0026gt; /usr/lib64/libsasl2.so.3 (0x00007f37633d6000) libbrotlicommon.so.1 =\u0026gt; /usr/lib64/libbrotlicommon.so.1 (0x00007f37631b5000) libkeyutils.so.1 =\u0026gt; /usr/lib64/libkeyutils.so.1 (0x00007f3762fb0000) libselinux.so.1 =\u0026gt; /lib64/libselinux.so.1 (0x00007f3762d87000) libpcre.so.1 =\u0026gt; /usr/lib64/libpcre.so.1 (0x00007f3762afe000) The output lists all shared libraries that must be copied into the final image, with one exception: linux-vdso.so.1. This shared library is not present on the file system, it is a virtual shared library exported by the kernel for improved performance. Consult the manpage of vdso via man vdso for more information about linux-vdso.so.1.\nThe remaining shared libraries are required. We will parse the output of ldd and copy them into a directory while maintaining their directory structure into the final distroless image.\nStart with copying the shared libraries that are linked by their name and not their full path as follows:\nfor lib in $(ldd /usr/bin/cargo | cut -d\u0026#34; \u0026#34; -f 3); do install -Dp $lib /l$lib done The code above copies all libraries, including their directory structure, into the directory /l/:\n# tree /l /l |-- lib64 | |-- libc.so.6 | |-- libcom_err.so.2 | |-- libdl.so.2 | |-- libgcc_s.so.1 | |-- libm.so.6 | |-- libpthread.so.0 | |-- libresolv.so.2 | |-- libselinux.so.1 | `-- libz.so.1 `-- usr `-- lib64 |-- libbrotlicommon.so.1 |-- libbrotlidec.so.1 |-- libcrypto.so.1.1 |-- libcurl.so.4 |-- libgssapi_krb5.so.2 |-- libidn2.so.0 |-- libjitterentropy.so.3 |-- libk5crypto.so.3 |-- libkeyutils.so.1 |-- libkrb5.so.3 |-- libkrb5support.so.0 |-- liblber-2.4.so.2 |-- libldap_r-2.4.so.2 |-- libnghttp2.so.14 |-- libpcre.so.1 |-- libpsl.so.5 |-- libsasl2.so.3 |-- libssh.so.4 |-- libssl.so.1.1 |-- libunistring.so.2 `-- libzstd.so.1 3 directories, 30 files We are still missing the shared libraries linked by their full path, in this case that is only /lib64/ld-linux-x86-64.so.2. We can copy them using the following snippet:\nfor lib in $(ldd /usr/bin/cargo | grep \u0026#34;^[[:space:]]*/\u0026#34; | cut -d\u0026#34; \u0026#34; -f 1); do install -Dp $lib /l$lib done This makes all necessary libraries available under /l/:\n# tree /l /l |-- lib64 | |-- ld-linux-x86-64.so.2 | |-- libc.so.6 | |-- libcom_err.so.2 | |-- libdl.so.2 | |-- libgcc_s.so.1 | |-- libm.so.6 | |-- libpthread.so.0 | |-- libresolv.so.2 | |-- libselinux.so.1 | `-- libz.so.1 `-- usr `-- lib64 |-- libbrotlicommon.so.1 |-- libbrotlidec.so.1 |-- libcrypto.so.1.1 |-- libcurl.so.4 |-- libgssapi_krb5.so.2 |-- libidn2.so.0 |-- libjitterentropy.so.3 |-- libk5crypto.so.3 |-- libkeyutils.so.1 |-- libkrb5.so.3 |-- libkrb5support.so.0 |-- liblber-2.4.so.2 |-- libldap_r-2.4.so.2 |-- libnghttp2.so.14 |-- libpcre.so.1 |-- libpsl.so.5 |-- libsasl2.so.3 |-- libssh.so.4 |-- libssl.so.1.1 |-- libunistring.so.2 `-- libzstd.so.1 3 directories, 31 files With the required information in place, you can create a Dockerfile. The following example uses the SLE BCI Base image as the base image, installs cargo and creates the directory tree shown above:\nFROM registry.suse.com/bci/bci-base:15.7 as builder RUN zypper -n in cargo RUN for lib in $(ldd /usr/bin/cargo | cut -d\u0026#34; \u0026#34; -f 3); do \\ install -Dp $lib /l$lib; \\ done RUN for lib in $(ldd /usr/bin/cargo | grep \u0026#34;^[[:space:]]*/\u0026#34; | cut -d\u0026#34; \u0026#34; -f 1); do \\ install -Dp $lib /l$lib; \\ done Next, copy cargo itself and the libraries under /l/ into an empty image based on SCRATCH. As this image only contains cargo, set both CMD and ENTRYPOINT to cargo, to prevent the container behaving unexpectedly when it is launched without parameters. The complete Dockerfile looks as follows:\nFROM registry.suse.com/bci/bci-base:15.7 as builder RUN zypper -n in cargo RUN for lib in $(ldd /usr/bin/cargo | cut -d\u0026#34; \u0026#34; -f 3); do \\ install -Dp $lib /l$lib; \\ done RUN for lib in $(ldd /usr/bin/cargo | grep \u0026#34;^[[:space:]]*/\u0026#34; | cut -d\u0026#34; \u0026#34; -f 1); do \\ install -Dp $lib /l$lib; \\ done FROM scratch COPY --from=builder /l/ / COPY --from=builder /usr/bin/cargo /usr/bin/cargo ENTRYPOINT [\u0026#34;/usr/bin/cargo\u0026#34;] Build the image with the preferred container runtime:\nDocker docker build -t cargo . Podman buildah bud --layers -t cargo . nerdctl nerdctl build -t cargo . This creates a fully containerized ready-to-use cargo container:\nDocker ❯ docker run --rm -it cargo help Rust\u0026#39;s package manager Usage: cargo [OPTIONS] [COMMAND] Options: -V, --version Print version info and exit --list List installed commands --explain \u0026lt;CODE\u0026gt; Run `rustc --explain CODE` -v, --verbose... Use verbose output (-vv very verbose/build.rs output) -q, --quiet Do not print cargo log messages --color \u0026lt;WHEN\u0026gt; Coloring: auto, always, never --frozen Require Cargo.lock and cache are up to date --locked Require Cargo.lock is up to date --offline Run without accessing the network --config \u0026lt;KEY=VALUE\u0026gt; Override a configuration value -Z \u0026lt;FLAG\u0026gt; Unstable (nightly-only) flags to Cargo, see \u0026#39;cargo -Z help\u0026#39; for details -h, --help Print help information Some common cargo commands are (see all commands with --list): build, b Compile the current package check, c Analyze the current package and report errors, but don\u0026#39;t build object files clean Remove the target directory doc, d Build this package\u0026#39;s and its dependencies\u0026#39; documentation new Create a new cargo package init Create a new cargo package in an existing directory add Add dependencies to a manifest file remove Remove dependencies from a manifest file run, r Run a binary or example of the local package test, t Run the tests bench Run the benchmarks update Update dependencies listed in Cargo.lock search Search registry for crates publish Package and upload this package to the registry install Install a Rust binary. Default location is $HOME/.cargo/bin uninstall Uninstall a Rust binary See \u0026#39;cargo help \u0026lt;command\u0026gt;\u0026#39; for more information on a specific command. Podman ❯ podman run --rm -it localhost/cargo help Rust\u0026#39;s package manager Usage: cargo [OPTIONS] [COMMAND] Options: -V, --version Print version info and exit --list List installed commands --explain \u0026lt;CODE\u0026gt; Run `rustc --explain CODE` -v, --verbose... Use verbose output (-vv very verbose/build.rs output) -q, --quiet Do not print cargo log messages --color \u0026lt;WHEN\u0026gt; Coloring: auto, always, never --frozen Require Cargo.lock and cache are up to date --locked Require Cargo.lock is up to date --offline Run without accessing the network --config \u0026lt;KEY=VALUE\u0026gt; Override a configuration value -Z \u0026lt;FLAG\u0026gt; Unstable (nightly-only) flags to Cargo, see \u0026#39;cargo -Z help\u0026#39; for details -h, --help Print help information Some common cargo commands are (see all commands with --list): build, b Compile the current package check, c Analyze the current package and report errors, but don\u0026#39;t build object files clean Remove the target directory doc, d Build this package\u0026#39;s and its dependencies\u0026#39; documentation new Create a new cargo package init Create a new cargo package in an existing directory add Add dependencies to a manifest file remove Remove dependencies from a manifest file run, r Run a binary or example of the local package test, t Run the tests bench Run the benchmarks update Update dependencies listed in Cargo.lock search Search registry for crates publish Package and upload this package to the registry install Install a Rust binary. Default location is $HOME/.cargo/bin uninstall Uninstall a Rust binary See \u0026#39;cargo help \u0026lt;command\u0026gt;\u0026#39; for more information on a specific command. nerdctl ❯ nerdctl run --rm -it cargo help Rust\u0026#39;s package manager Usage: cargo [OPTIONS] [COMMAND] Options: -V, --version Print version info and exit --list List installed commands --explain \u0026lt;CODE\u0026gt; Run `rustc --explain CODE` -v, --verbose... Use verbose output (-vv very verbose/build.rs output) -q, --quiet Do not print cargo log messages --color \u0026lt;WHEN\u0026gt; Coloring: auto, always, never --frozen Require Cargo.lock and cache are up to date --locked Require Cargo.lock is up to date --offline Run without accessing the network --config \u0026lt;KEY=VALUE\u0026gt; Override a configuration value -Z \u0026lt;FLAG\u0026gt; Unstable (nightly-only) flags to Cargo, see \u0026#39;cargo -Z help\u0026#39; for details -h, --help Print help information Some common cargo commands are (see all commands with --list): build, b Compile the current package check, c Analyze the current package and report errors, but don\u0026#39;t build object files clean Remove the target directory doc, d Build this package\u0026#39;s and its dependencies\u0026#39; documentation new Create a new cargo package init Create a new cargo package in an existing directory add Add dependencies to a manifest file remove Remove dependencies from a manifest file run, r Run a binary or example of the local package test, t Run the tests bench Run the benchmarks update Update dependencies listed in Cargo.lock search Search registry for crates publish Package and upload this package to the registry install Install a Rust binary. Default location is $HOME/.cargo/bin uninstall Uninstall a Rust binary See \u0026#39;cargo help \u0026lt;command\u0026gt;\u0026#39; for more information on a specific command. ","description":" What is a distroless image? Distroless images are stripped down container images, where the underlying Linux distribution is reduced to the bare minimum. A distroless image normally contains only certificates and specific core libraries, and it does not include a shell or utilities like cat or ls.\nThe advantages of distroless images include smaller size and potentially fewer vulnerabilities.\nThe major disadvantage is the difficulty of debugging a containerized application, as the container image does not provide any debugging tools and may even lack tools to read log files. Debugging applications usually requires attaching sidecar containers or interacting with the container via the /proc/ filesystem. For this reason, we recommend to use SLE BCI Base, SLE BCI Micro or SLE BCI BusyBox as the deployment image, because they make debugging easier, and they come with a valid rpm database.\n"},{"id":12,"href":"/bci-docs/guides/container-suseconnect/","title":"How to use container-suseconnect","parent":"Guides","content":" What is container-suseconnect? container-suseconnect is a plugin available in all Base Container Images that ship with Zypper. When the plugin detects the host’s SUSE Linux Enterprise Server registration credentials, it uses them to give the container access the SUSE Linux Enterprise repositories. This includes additional modules and previous package versions that are not part of the free SLE_BCI repository.\nHow to use container-suseconnect If you are running a registered SLES system with Docker, container-suseconnect automatically detects and uses the subscription, without requiring any action on your part.\nOn openSUSE systems with Docker, you must copy the files /etc/SUSEConnect and /etc/zypp/credentials.d/SCCcredentials from a registered SLES machine to your local machine. Note that the /etc/SUSEConnect file is required only if you are using RMT for managing your registration credentials.\nHow to use container-suseconnect on non-SLE hosts or with Podman, Buildah or with nerdctl You need a registered SLES system to use container-suseconnect on non-SLE hosts or with Podman, Buildah, or with nerdctl. This can be a physical machine, a virtual machine, or the bci-base container with SUSEConnect installed and registered.\nIf you don’t use RMT, copy /etc/zypp/credentials.d/SCCcredentials to the development machine. Otherwise, copy both the /etc/zypp/credentials.d/SCCcredentials and /etc/SUSEConnect files.\nYou can use the following command to obtain SCCcredentials (replace REGISTRATION_CODE with your SCC registration code)\nDocker docker run --rm registry.suse.com/suse/sle15:latest bash -c \\ \u0026#34;zypper -n in SUSEConnect; SUSEConnect --regcode REGISTRATION_CODE; \\ cat /etc/zypp/credentials.d/SCCcredentials\u0026#34; Podman podman run --rm registry.suse.com/suse/sle15:latest bash -c \\ \u0026#34;zypper -n in SUSEConnect; SUSEConnect --regcode REGISTRATION_CODE; \\ cat /etc/zypp/credentials.d/SCCcredentials\u0026#34; nerdctl nerdctl run --rm registry.suse.com/suse/sle15:latest bash -c \\ \u0026#34;zypper -n in SUSEConnect; SUSEConnect --regcode REGISTRATION_CODE; \\ cat /etc/zypp/credentials.d/SCCcredentials\u0026#34; If you are running a container based on a SLE BCI, mount SCCcredentials (and optionally /etc/SUSEConnect) in the correct destination. The following example shows how to mount SCCcredentials in the current working directory:\nDocker docker run -v /path/to/SCCcredentials:/etc/zypp/credentials.d/SCCcredentials \\ -it --pull=always registry.suse.com/bci/bci-base:latest Podman podman run -v /path/to/SCCcredentials:/etc/zypp/credentials.d/SCCcredentials \\ -it --pull=always registry.suse.com/bci/bci-base:latest nerdctl nerdctl run -v /path/to/SCCcredentials:/etc/zypp/credentials.d/SCCcredentials \\ -it --pull=always registry.suse.com/bci/bci-base:latest Do not copy the SCCcredentials and SUSEConnect files into the container image to avoid inadvertently adding them to the final image. Use secrets instead, as they are only available to a single layer and are not part of the built image. To do this, put a copy of SCCcredentials (and optionally SUSEConnect) somewhere on the file system and modify the RUN instructions that invoke Zypper as follows:\nFROM registry.suse.com/bci/bci-base:latest RUN --mount=type=secret,id=SUSEConnect \\ --mount=type=secret,id=SCCcredentials \\ zypper -n in fluxbox Docker and Buildah both support mounting secrets via the --secret flag as follows: Docker docker build --secret=id=SCCcredentials,src=/path/to/SCCcredentials \\ --secret=id=SUSEConnect,src=/path/to/SUSEConnect . Podman buildah bud --layers --secret=id=SCCcredentials,src=/path/to/SCCcredentials \\ --secret=id=SUSEConnect,src=/path/to/SUSEConnect . Adding modules into the container or container Image container-suseconnect allows you to automatically add SLE Modules into a container or container image. What modules are added is determined by the environment variable ADDITIONAL_MODULES that includes a comma-separated list of the module names. In a Dockerfile, this is done using the ENV directive as follows:\nFROM registry.suse.com/bci/bci-base:latest ENV ADDITIONAL_MODULES sle-module-desktop-applications,sle-module-development-tools RUN --mount=type=secret,id=SCCcredentials zypper -n in fluxbox \u0026amp;\u0026amp; zypper -n clean ","description":" What is container-suseconnect? container-suseconnect is a plugin available in all Base Container Images that ship with Zypper. When the plugin detects the host’s SUSE Linux Enterprise Server registration credentials, it uses them to give the container access the SUSE Linux Enterprise repositories. This includes additional modules and previous package versions that are not part of the free SLE_BCI repository.\nHow to use container-suseconnect If you are running a registered SLES system with Docker, container-suseconnect automatically detects and uses the subscription, without requiring any action on your part.\n"},{"id":13,"href":"/bci-docs/guides/vscode-dev-containers/","title":"How To Use SLE BCIs As VScode Development Containers","parent":"Guides","content":" VS Code with the source mounted in a Development Container. The open terminal is a console within the container. Visual Studio Code has a feature called Development Containers. This is part of the built-in functionality to work with remote containers. The Language Stack SLE BCIs make a great choice to use as a development environment.\nWhy Use The SLE BCI In Development Containers There are two reasons the BCI language stack containers are useful for development.\nFirst, you develop in the same environment as when you use a Language Stack SLE BCI to build or run your application. Being able to develop and build or run your application in the same environment setup enables you to discover quirks and issues that are related to the way your application works in the environment.\nSecond, when you have a team of people working on an application, all developers will have the same environment to work in. Whether they are on Windows, macOS, or Linux their will be the same.\nDocker Socket Required VS Code creates the development containers using the Docker Engine and it communicates with it over the Docker socket. That means you need a Docker socket available on your system.\nRancher Desktop is our recommended app for working with containers. It is available for Windows, macOS, and Linux. Alternatively, you can use another tool such as Docker Desktop or the Docker Engine itself if you are using Linux.\nNote VS Code mounts your code from the local system inside the container. This works best when the container runtime is on your local system as opposed to remote. While possible to do this with the container runtime on a remote system, this guide does not cover running the development container on a remote machine. Development Container Basics Development containers enable you to mount any folder inside a container where you can specify the environment. Debugging, extensions, and other features work with the code as it is mounted within the container rather than the location on the local file system.\nDevelopment Container Configuration The configuration for Development Containers is stored in a file named .devcontainer/devcontainer.json. When you open up a codebase with this configuration file present, VS Code will read the configuration and present you with an option to use a Development Container.\nThere are two ways to specify where to get the container from. You can point it at an image or you can point it at a Dockerfile to build the image from. Since VS Code needs some additional packages installed you can’t simply point it at a SLE BCI image.\nTo illustrate using the Dockerfile method we can look at a setup for the Go programming language. A Dockerfile placed in the .devcontainer directory would look like:\nFROM registry.suse.com/bci/golang:stable # Install tools needed by Visual Studio Code Remote Development Containers RUN zypper --non-interactive install -y tar git gzip VS Code needs git, gzip, and tar installed which are not present in the Go SLE BCI image by default.\nWhile this example is targeted at Go, it will work for other languages where there is a BCI language stack available.\nThis file needs to be referenced in the devcontainer/devcontainer.json file. For example, a devcontainer.json could look like the following:\n{ \u0026#34;name\u0026#34;: \u0026#34;Golang\u0026#34;, \u0026#34;build\u0026#34;: { \u0026#34;dockerfile\u0026#34;: \u0026#34;Dockerfile\u0026#34; } } If you open up a project with these files in them VS Code will prompt you to open them in a container as the image below illustrates.\nPop-up asking if you want to open the code within a Development Container. This example JSON configuration file is in its simplest form. You can learn more details about the additional configuration in the VS Code documentation for Development Containers.\n","description":" VS Code with the source mounted in a Development Container. The open terminal is a console within the container. Visual Studio Code has a feature called Development Containers. This is part of the built-in functionality to work with remote containers. The Language Stack SLE BCIs make a great choice to use as a development environment.\n"},{"id":14,"href":"/bci-docs/documentation/bci-images/","title":"Images Available","parent":"Documentation","content":" SUSE Linux Base Container Images (SUSE Linux BCI) provide three types of container images:\nGeneral-purpose images allow building custom container images and deploying applications.\nLanguage stack images allow developing and deploying applications in specific programming languages.\nApplication stack images feature popular containerized applications like Nginx, PostgreSQL, and MariaDB.\nGeneral-purpose The general-purpose container images come with a minimum set of packages to keep their size small. You can use it as a starting point for building other container images, or to deploy specific software using the free BCI repository.\nFor more information, see the general-purpose guide.\nLanguage stack The language stack container images include the most common tools for building and deploying applications in their specific language environment. This includes tools like a compiler or interpreter, as well as the language-specific package manager.\nIt is possible to customize the image using the free BCI repository.\nFor more information, see the language stack guide.\nApplication stack The application stack container images offer production-ready applications. These container images don’t include a package manager or the free BCI repository.\nFor more information, see the application stack guide.\n","description":" SUSE Linux Base Container Images (SUSE Linux BCI) provide three types of container images:\nGeneral-purpose images allow building custom container images and deploying applications.\nLanguage stack images allow developing and deploying applications in specific programming languages.\nApplication stack images feature popular containerized applications like Nginx, PostgreSQL, and MariaDB.\nGeneral-purpose The general-purpose container images come with a minimum set of packages to keep their size small. You can use it as a starting point for building other container images, or to deploy specific software using the free BCI repository.\n"},{"id":15,"href":"/bci-docs/","title":"Introduction to SUSE Linux Base Container Images","parent":"","content":" SUSE Linux Base Container Images (SUSE Linux BCI) provide a set of container images that are truly open, flexible and secure. They are designed to provide the foundation for your cloud native applications, eneabling you to build, deploy and share freely distributable SLES-based container images provided by SUSE.\nSUSE Linux BCI is based on SUSE Linux Enterprise Server (SLES) 15 and 16. It is compatible with SLES without a subscription. Likewise, it features the same predictable enterprise lifecycle as SLES. The free BCI repository (which is a subset of the SLES repository) provides access to over 4,000 packages. The packages in this repository undergo quality assurance and security audits by SUSE.\nAdditionally, SUSE offers L3 support for SUSE Linux BCI through SUSE subscription plans.\nSecurity To ensure a secure supply chain, SUSE signs all container images. Packages in the free BCI repository undergo the same security audits as SLES. Likewise, SUSE Linux BCI benefits from the same CVE mitigation as SLES.\nStability Since SUSE Linux BCI is based on SLES, it features the same level of stability, quality assurance, and enterprise lifecycle. The container images receive regular maintenance updates. These updates provide bug fixes, improvements and security patches.\nIntegration SUSE Linux BCI works on any OCI-compliant runtime and Linux OS. No vendor lock-in. Drop-in replacements are available for selected container images from Docker Hub.\nRedistribution The SUSE Linux BCI EULA allows you to redistribute container images based on SUSE Linux BCI. No subscription required; ideal for development and production deployments.\n","description":" SUSE Linux Base Container Images (SUSE Linux BCI) provide a set of container images that are truly open, flexible and secure. They are designed to provide the foundation for your cloud native applications, eneabling you to build, deploy and share freely distributable SLES-based container images provided by SUSE.\nSUSE Linux BCI is based on SUSE Linux Enterprise Server (SLES) 15 and 16. It is compatible with SLES without a subscription. Likewise, it features the same predictable enterprise lifecycle as SLES. The free BCI repository (which is a subset of the SLES repository) provides access to over 4,000 packages. The packages in this repository undergo quality assurance and security audits by SUSE.\n"},{"id":16,"href":"/bci-docs/documentation/language-stack-bci/","title":"Language Stack Images","parent":"Documentation","content":" The language stack container images include the most common tools for building and deploying applications in their specific language environment. This includes tools like a compiler or interpreter, as well as the language-specific package manager.\nThese images are built on top of SUSE Linux BCI Base and have the free BCI repository pre-configured.\nA complete list of images is available here.\nC/C++ Image repository: registry.suse.com/bci/gcc\nThe C/C++ image includes the GNU Compiler Collection version specified in the tag, as well as cURL, Git, GNU make and Fortran.\nGo Image repository: registry.suse.com/bci/golang\nThe Go image includes the Go compiler version specified in the tag, as well as cURL, Git and GNU make.\nNode.js Image repository: registry.suse.com/bci/nodejs\nThe Node.js image includes the Node.js version specified in the tag, as well as cURL, Git and npm.\nYou can install Yarn with the following command:\nnpm install -g yarn OpenJDK Image repository:\nRuntime: registry.suse.com/bci/openjdk\nDevelopment: registry.suse.com/bci/openjdk-devel\nThe OpenJDK images include the OpenJDK Runtime version specified in the tag, as well as cURL and Git.\nThe development image also includes the OpenJDK Development Environment and Maven.\nPython Image repository: registry.suse.com/bci/python\nThe Python image includes the Python version specified in the tag, as well as cURL, Git and Pip. Pipx and wheel are also available on selected versions.\nPHP Image repository:\nPHP: registry.suse.com/bci/php\nPHP with Apache: registry.suse.com/bci/php-apache\nPHP with FPM: registry.suse.com/bci/php-fpm\nThe PHP image includes the PHP version specified in the tag, as well as Composer and selected PHP extensions.\nThe Apache variant also includes the Apache HTTP Server.\nThe FPM variant also includes the FastCGI Process Manager (FPM)\nRuby Image repository: registry.suse.com/bci/ruby\nThe Ruby image includes the Ruby version specified in the tag, as well as cURL, Git, Bundler and Gem. It also includes Rails dependencies, such as C++, GNU make and SQLite.\nRust Image repository: registry.suse.com/bci/rust\nThe Rust image includes the Rust and Cargo versions specified in the tag.\n.NET Image repository:\nRuntime: registry.suse.com/bci/dotnet-runtime\nSDK: registry.suse.com/suse/dotnet-sdk\nASP.NET Core Runtime: registry.suse.com/bci/dotnet-aspnet\nThe .NET images include the .NET version specified in the tag.\nMicrosoft provides the .NET packages through the Microsoft package feed. As the .NET packages in this image are from a third-party repository, SUSE doesn’t provide support or warranties for such packages.\n","description":" The language stack container images include the most common tools for building and deploying applications in their specific language environment. This includes tools like a compiler or interpreter, as well as the language-specific package manager.\nThese images are built on top of SUSE Linux BCI Base and have the free BCI repository pre-configured.\nA complete list of images is available here.\nC/C++ Image repository: registry.suse.com/bci/gcc\nThe C/C++ image includes the GNU Compiler Collection version specified in the tag, as well as cURL, Git, GNU make and Fortran.\n"},{"id":17,"href":"/bci-docs/guides/podman-generate-systemd/","title":"Launch Containers with Podman and Systemd","parent":"Guides","content":" Local Container Orchestration A container runtime makes it easy to launch an application distributed as a single container. But things get more complicated when you need to run applications consisting of multiple containers, or when it’s necessary to start the applications automatically on system boot and restart them after they crash. While container orchestration tools like Kubernetes are designed for that exact purpose, they are intended to be used for highly distributed and scalable systems with hundreds of nodes, and not for a single machine. systemd and Podman are much better suited for the single-machine scenario, as they do not add another layer complexity to your existing setup.\nOverview Starting with version 1.3.0, Podman supports creating systemd unit files with the podman generate systemd subcommand. The subcommand creates a systemd unit file, making it possible to control a container or pod via systemd. Using the unit file you can launch a container or pod on boot, automatically restart it if a failure occurs, and keep its logs in journald.\nCreating a new Systemd Unit File The following example uses a simple NGINX container:\n❯ podman run -d --name web -p 8080:80 docker.io/nginx c0148d8476418a2da938a711542c55efc09e4119909aea70e287465c6fb51618 Generating a systemd unit for the container can be done as follows:\n❯ podman generate systemd --name --new web # container-web.service # autogenerated by Podman 4.2.0 # Tue Sep 13 10:58:54 CEST 2022 [Unit] Description=Podman container-web.service Documentation=man:podman-generate-systemd(1) Wants=network-online.target After=network-online.target RequiresMountsFor=%t/containers [Service] Environment=PODMAN_SYSTEMD_UNIT=%n Restart=on-failure TimeoutStopSec=70 ExecStartPre=/bin/rm -f %t/%n.ctr-id ExecStart=/usr/bin/podman run \\ --cidfile=%t/%n.ctr-id \\ --cgroups=no-conmon \\ --rm \\ --sdnotify=conmon \\ --replace \\ -d \\ --name web \\ -p 8080:80 docker.io/nginx ExecStop=/usr/bin/podman stop --ignore --cidfile=%t/%n.ctr-id ExecStopPost=/usr/bin/podman rm -f --ignore --cidfile=%t/%n.ctr-id Type=notify NotifyAccess=all [Install] WantedBy=default.target Podman outputs a unit file to the console that can be put either into the user unit systemd directories (~/.config/systemd/user/ or /etc/systemd/user/) or into the system unit systemd directory (/etc/systemd/systtem) and control the container via systemd. The --new flag instructs Podman to recreate the container on a restart. This ensures that the systemd unit is self-contained, and it does not depend on external state. The --name flag allows you to assign a user-friendly name to the container: without it Podman uses container IDs instead of their names.\nTo control the container as a user unit, proceed as follows:\n❯ podman generate systemd --name --new --files web /home/user/container-web.service ❯ mv container-web.service ~/.config/systemd/user/ ❯ systemctl --user daemon-reload Now the container can be started via systemctl --user start container-web:\n❯ systemctl --user start container-web ❯ systemctl --user is-active container-web.service active Run the podman ps command to see the list of all running containers :\n❯ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES af92743971d2 docker.io/library/nginx:latest nginx -g daemon o... 15 minutes ago Up 15 minutes ago 0.0.0.0:8080-\u0026gt;80/tcp web One of the benefits of managing the container via systemd is the ability to automatically restart the container if it crashes. You can simulate a crash by sending SIGKILL to the main process in the container:\n❯ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4c89582fa9cb docker.io/library/nginx:latest nginx -g daemon o... About a minute ago Up About a minute ago 0.0.0.0:8080-\u0026gt;80/tcp web ❯ kill -9 $(podman inspect --format \u0026#34;{{.State.Pid}}\u0026#34; web) ❯ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 0b5be4493251 docker.io/library/nginx:latest nginx -g daemon o... 4 seconds ago Up 4 seconds ago 0.0.0.0:8080-\u0026gt;80/tcp web Note that the container is not restarted when it is stopped gracefully, e.g. via podman stop web. To always restart it, add the flag --restart-policy=always to podman generate systemd.\nUpdating container images Using the described approach means that the container image is never updated. You can solve the problem by adding the --pull=always flag to the ExecStart= entry in the unit file. But be aware that this increases the startup time of the container and updates the image on every restart. The latter also means that a container image update can make the container unavailable outside of a scheduled maintenance window due to a newly introduced bug.\nThe auto-update subcommand in Podman provides a possible solution. Add the label io.containers.autoupdate=registry to a container to make Podman pull a new version of the container image from the registry when running podman auto-update. This makes it possible to update all container images with a single command at a desired time, and without increasing the startup time of the systemd units.\nThe auto update feature can be enabled by adding the line --label \u0026#34;io.containers.autoupdate=registry\u0026#34; \\ to the ExecStart= entry of the container’s systemd unit file. For the NGINX example, modify ~/.config/systemd/user/container-web.service as follows:\nExecStart=/usr/bin/podman run \\ --cidfile=%t/%n.ctr-id \\ --cgroups=no-conmon \\ --rm \\ --sdnotify=conmon \\ --replace \\ -d \\ --name web \\ --label \u0026#34;io.containers.autoupdate=registry\u0026#34; \\ -p 8080:80 docker.io/nginx After reloading the daemons and restarting the container, perform a dry run of the update (it will most likely not report any updates):\n❯ podman auto-update --dry-run UNIT CONTAINER IMAGE POLICY UPDATED container-web.service 87d263489307 (web) docker.io/nginx registry false It is good practice to have external testing in place to make sure that image updates are generally safe to be deployed. If you are confident in the quality of our container image, you can let Podman automatically apply image updates periodically by enabling the podman-auto-update.timer:\n# just for the current user ❯ systemctl --user enable podman-auto-update.timer Created symlink /home/user/.config/systemd/user/timers.target.wants/podman-auto-update.timer → /usr/lib/systemd/user/podman-auto-update.timer. # or as root ❯ sudo systemctl enable podman-auto-update.timer Created symlink /etc/systemd/system/timers.target.wants/podman-auto-update.timer → /usr/lib/systemd/system/podman-auto-update.timer. Managing multiple containers Certain applications rely on more than one container to function, for example a web frontend, a backend server and a database. Docker compose is popular tool for deploying multi-container applications on a single machine. While Podman does not support the compose command natively, in most cases compose files can be ported to a Podman pod and multiple containers.\nThe following example deploys a Drupal and PostgreSQL container in a single pod and manages these via systemd units. First, create a new pod that exposes the Drupal web interface:\n❯ podman pod create -p 8080:80 --name drupal 736cab072c49e68ad368ba819e9117be13ef8fa048a2eb88736b5968b3a19a64 Once the pod has been created, launch the Drupal frontend and the PostgreSQL database inside it:\n❯ podman run -d --name drupal-frontend --pod drupal docker.io/drupal ffd2fbd6d445e63fb0c28abb8d25ced78f819211d3bce9d6174fe4912d89f0ca ❯ podman run -d --name drupal-pg --pod drupal \\ -e POSTGRES_DB=drupal \\ -e POSTGRES_USER=user \\ -e POSTGRES_PASSWORD=pass \\ docker.io/postgres:11 a4dc31b24000780d9ffd81a486d0d144c47c3adfbecf0f7effee24a00273fcde This results in three running containers: the Drupal web interface, the PostgreSQL database and the pod’s infrastructure container.\n❯ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2948fa1476c6 localhost/podman-pause:4.2.0-1660228937 2 minutes ago Up About a minute ago 0.0.0.0:8080-\u0026gt;80/tcp 736cab072c49-infra ffd2fbd6d445 docker.io/library/drupal:latest apache2-foregroun... About a minute ago Up About a minute ago 0.0.0.0:8080-\u0026gt;80/tcp drupal-frontend a4dc31b24000 docker.io/library/postgres:11 postgres 40 seconds ago Up 41 seconds ago 0.0.0.0:8080-\u0026gt;80/tcp drupal-pg Creating a systemd unit for the pod is done similar to a single container:\n❯ podman generate systemd --name --new --files drupal /home/user/pod-drupal.service /home/user/container-drupal-frontend.service /home/user/container-drupal-pg.service ❯ mv *service ~/.config/systemd/user/ ❯ systemctl daemon-reload --user Since Podman is aware of which containers belong to the drupal pod and how their systemd units are called, it can correctly add the dependencies to the pod’s unit file. This means that when you start or stop the pod, systemd ensures that all containers inside the pod are started or stopped automatically.\nTo check systemd’s dependency handling, first stop the drupal pod and verify that no containers are currently running on the host:\n❯ podman pod stop drupal 736cab072c49e68ad368ba819e9117be13ef8fa048a2eb88736b5968b3a19a64 ❯ podman pod rm drupal 736cab072c49e68ad368ba819e9117be13ef8fa048a2eb88736b5968b3a19a64 ❯ podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Start the drupal pod via systemctl start --user pod-drupal.service, and systemd launches the containers inside the pod:\n❯ systemctl start --user pod-drupal.service ❯ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d1589d3ac68b localhost/podman-pause:4.2.0-1660228937 5 seconds ago Up 5 seconds ago 0.0.0.0:8080-\u0026gt;80/tcp ca41b505bd13-infra a49bea53c20c docker.io/library/postgres:11 postgres 4 seconds ago Up 5 seconds ago 0.0.0.0:8080-\u0026gt;80/tcp drupal-pg dc9dca018dad docker.io/library/drupal:latest apache2-foregroun... 4 seconds ago Up 5 seconds ago 0.0.0.0:8080-\u0026gt;80/tcp drupal-frontend ","description":" Local Container Orchestration A container runtime makes it easy to launch an application distributed as a single container. But things get more complicated when you need to run applications consisting of multiple containers, or when it’s necessary to start the applications automatically on system boot and restart them after they crash. While container orchestration tools like Kubernetes are designed for that exact purpose, they are intended to be used for highly distributed and scalable systems with hundreds of nodes, and not for a single machine. systemd and Podman are much better suited for the single-machine scenario, as they do not add another layer complexity to your existing setup.\n"},{"id":18,"href":"/bci-docs/tags/","title":"Tags","parent":"Introduction to SUSE Linux Base Container Images","content":"","description":""},{"id":19,"href":"/bci-docs/guides/using-sle-bci/","title":"Using SLE BCI","parent":"Guides","content":" Package manager The default package manager in SUSE Linux Enterprise is Zypper. Similar to APT in Debian and APK in Alpine Linux, Zypper offers a command-line interface for all package management tasks. Below is brief overview of commonly used container-related Zypper commands.\nInstall packages zypper --non-interactive install $PACKAGE_NAME Add a repository zypper --non-interactive addrepo $REPOSITORY_URL; zypper --non-interactive refresh Update all packages zypper --non-interactive update Remove a package zypper --non-interactive remove --clean-deps $PACKAGE_NAME the --clean-deps flag ensures that no longer required dependencies are removed as well\nClean up temporary files zypper clean For more information on using Zypper, refer to the zypper documentation.\nAll the described commands use the --non-interactive flag to skip confirmations, since you cannot approve these manually during container builds. Keep in mind that you must use the flag with any command that modifies the system. Also note that --non-interactive is not a \u0026#34;yes to all\u0026#34; flag. Instead, --non-interactive confirms what is considered to be the intention of the user. For example, an installation command with the --non-interactive option fails if it needs to import new repository signing keys, as that is something that the user should verify themselves.\nCommon patterns Here are a few examples that can give you an idea how to accomplish certain tasks in SLE BCI compared to Debian.\nRemove orphaned packages Debian: apt-get autoremove -y\nSLE BCI: Not required as long as you remove installed packages using the zypper --non-interactive remove --clean-deps $PACKAGE_NAME\nObtain container’s architecture Debian: dpkgArch=\u0026#34;$(dpkg --print-architecture | awk -F- \u0026#39;{ print $NF }\u0026#39;)\u0026#34;\nSLE BCI: arch=\u0026#34;$(uname -p|sed \u0026#39;s/x86_64/amd64/\u0026#39;)\u0026#34;\nInstall packages required for compilation Debian: apt-get install -y build-essential\nSLE BCI: zypper -n in gcc gcc-c++ make\nVerify GnuPG signatures Debian: gpg --batch --verify $SIGNATURE_URL $FILE_TO_VERIFY\nSLE BCI: zypper -n in dirmngr; gpg --batch --verify $SIGNATURE_URL $FILE_TO_VERIFY; zypper -n remove --clean-deps dirmngr; zypper -n clean\nPackage naming conventions SUSE Linux Enterprise package naming conventions differ from Debian, Ubuntu, and Alpine, and they are closer to those of Red Hat Enterprise Linux. The main difference is that development packages of libraries (that is, packages containing headers and build description files) are named $PACKAGE-devel in SUSE Linux Enterprise, as opposed to $PACKAGE-dev as they are in Debian and Ubuntu. When in doubt, search for the package directly using the following command:\nDocker docker run --rm registry.suse.com/bci/bci-base:$OS_VERSION zypper search $PACKAGE_NAME Podman podman run --rm registry.suse.com/bci/bci-base:$OS_VERSION zypper search $PACKAGE_NAME nerdctl nerdctl run --rm registry.suse.com/bci/bci-base:$OS_VERSION zypper search $PACKAGE_NAME (replace OS_VERSION with the appropriate service version number, for example: 15.3 or 15.4).\nAdding GPG signing keys Adding external repositories to a container or container image normally requires importing the GPG key used for signing the packages. This can be done with the rpm --import $KEY_URL command. This adds the key to the RPM database, and all packages from the repository can be installed afterwards.\n","description":" Package manager The default package manager in SUSE Linux Enterprise is Zypper. Similar to APT in Debian and APK in Alpine Linux, Zypper offers a command-line interface for all package management tasks. Below is brief overview of commonly used container-related Zypper commands.\nInstall packages zypper --non-interactive install $PACKAGE_NAME Add a repository zypper --non-interactive addrepo $REPOSITORY_URL; zypper --non-interactive refresh Update all packages zypper --non-interactive update Remove a package zypper --non-interactive remove --clean-deps $PACKAGE_NAME "},{"id":20,"href":"/bci-docs/guides/verify-sle-bci/","title":"Verify SLE Base Container Images","parent":"Guides","content":" Introduction Verifying container images allows you to confirm their provenance, thus ensuring the supply chain security. This document demonstrates how to verify container images using Cosign and how to integrate the verification step into your Podman installation.\nVerifying SLE BCI with Cosign To verify a SLE BCI image, run Cosign in the container. The command below fetches the signing key from the SUSE server and uses it to verify the latest BCI-Base container image.\n\u0026gt; podman run --rm -it gcr.io/projectsigstore/cosign verify \\ --key https://ftp.suse.com/pub/projects/security/keys/container-key.pem \\ registry.suse.com/bci/bci-base:latest | tail -1 | jq [ { \u0026#34;critical\u0026#34;: { \u0026#34;identity\u0026#34;: { \u0026#34;docker-reference\u0026#34;: \u0026#34;registry.suse.com/bci/bci-base\u0026#34; }, \u0026#34;image\u0026#34;: { \u0026#34;docker-manifest-digest\u0026#34;: \u0026#34;sha256:52a828600279746ef669cf02a599660cd53faf4b2430a6b211d593c3add047f5\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;cosign container image signature\u0026#34; }, \u0026#34;optional\u0026#34;: { \u0026#34;creator\u0026#34;: \u0026#34;OBS\u0026#34; } } ] The signing key can be used to verify all SLE BCI container images, and it also ships with SLE 15 (the /usr/share/container-keys/suse-container-key.pem file).\nYou can also check SLE BCI container images against rekor, the immutable tamper resistant ledger. For example:\n\u0026gt; podman run --rm -it -e COSIGN_EXPERIMENTAL=1 gcr.io/projectsigstore/cosign \\ verify --key https://ftp.suse.com/pub/projects/security/keys/container-key.pem \\ registry.suse.com/bci/bci-base:latest | tail -1 | jq [ { \u0026#34;critical\u0026#34;: { \u0026#34;identity\u0026#34;: { \u0026#34;docker-reference\u0026#34;: \u0026#34;registry.suse.com/bci/bci-base\u0026#34; }, \u0026#34;image\u0026#34;: { \u0026#34;docker-manifest-digest\u0026#34;: \u0026#34;sha256:52a828600279746ef669cf02a599660cd53faf4b2430a6b211d593c3add047f5\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;cosign container image signature\u0026#34; }, \u0026#34;optional\u0026#34;: { \u0026#34;creator\u0026#34;: \u0026#34;OBS\u0026#34; } } ] If verification fails, the output of the cosign verify command is similar to the one below.\nError: no matching signatures: crypto/rsa: verification error main.go:62: error during command execution: no matching signatures: crypto/rsa: verification error Verifying SLE BCI with Podman There are three main prerequisites to verify SLE Base Container Images using Podman. First, specify registry.suse.com as the registry for which image verification will be enabled.\nUpdate Podman registries configuration Note Skip this step on SLE or openSUSE, as the correct configuration is already in place. Add the following configuration to /etc/containers/registries.d/default.yaml:\ndocker: registry.suse.com: use-sigstore-attachments: true Instead of editing the default.yaml, you can create a new file in /etc/containers/registries.d/ with a filename of your choice.\nUpdate signature verification policy (/etc/containers/policy.json) Next, modify the /etc/containers/policy.json file. Under the docker attribute, add the registry.suse.com configuration similar to the following:\n{ \u0026#34;default\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;insecureAcceptAnything\u0026#34; } ], \u0026#34;transports\u0026#34;: { \u0026#34;docker-daemon\u0026#34;: { \u0026#34;\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;insecureAcceptAnything\u0026#34; } ] }, \u0026#34;docker\u0026#34;: { \u0026#34;registry.suse.com\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;sigstoreSigned\u0026#34;, \u0026#34;keyPath\u0026#34;: \u0026#34;/usr/share/pki/containers/suse-container-key.pem\u0026#34;, \u0026#34;signedIdentity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;matchRepository\u0026#34; } } ] } } } The specified configuration instructs Podman, Skopeo and Buildah to verify images under the registry.suse.com repository. This way, before pulling the image, Podman checks the validity of the signature using the specified public key, and rejects the image if the validation fails.\nNote Do not remove existing entries in transports.docker. Instead append the entry for registry.suse.com to the list. Fetch the SUSE Container signing key Note This step is optional on SLE. The signing key is already installed under /usr/share/pki/containers/suse-container-key.pem. Fetch the public key used to sign SLE BCIs from SUSE Signing Keys, or use the following command:\n\u0026gt; sudo curl -s https://ftp.suse.com/pub/projects/security/keys/container-key.pem \\ -o /usr/share/pki/containers/suse-container-key.pem Verifying if the image is signed Buildah, Podman and Skopeo will automatically verify every image pulled from registry.suse.com from now on. There are no additional steps required.\nIf verification fails, the command returns an error message as follows:\n\u0026gt; podman pull registry.suse.com/bci/bci-base:latest Trying to pull registry.suse.com/bci/bci-base:latest... Error: copying system image from manifest list: Source image rejected: Signature for identity registry.suse.com/bci/bci-base is not accepted If there are no issues with the signed image and your configuration, you can continue with your usual development workflow.\n","description":" Introduction Verifying container images allows you to confirm their provenance, thus ensuring the supply chain security. This document demonstrates how to verify container images using Cosign and how to integrate the verification step into your Podman installation.\nVerifying SLE BCI with Cosign To verify a SLE BCI image, run Cosign in the container. The command below fetches the signing key from the SUSE server and uses it to verify the latest BCI-Base container image.\n"}]