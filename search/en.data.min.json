[{"id":0,"href":"/bci-docs/guides/adding-users/","title":"Adding Users to SLE BCI Micro and Minimal","parent":"Guides","content":" Note This guide will demonstrate how to add users to the SLE BCI Micro and SLE BCI Minimal images, without having the useradd binary installed. Background The SLE BCI Micro and Minimal images are tailored towards providing a small footprint and thus do not ship the useradd binary. While this reduces the image size, creating new users inside containers based on BCI Micro or Minimal involves a few additional steps.\nSwitch to using the BusyBox SLE BCI SLE BCI Minimal and SLE BCI Micro are lightweight deployment images without a package manager and tailored for specific use cases. If you do not require a package manager in your final image and additionally:\nyou do not need rpm\nyour application runs with POSIX sh and not just Bash\nThen consider using the SLE BCI BusyBox image instead. It is even smaller than SLE BCI Micro and ships the BusyBox implementation of useradd. Adding a new user in BusyBox is straightforward:\nFROM registry.suse.com/bci/bci-busybox:15.4 ARG user # add -H if /home/$user shall not be created RUN adduser -D $user USER $user This container can be built using your favorite container runtime as follows: Docker docker build --build-arg user=rancher . Podman buildah bud --layers --build-arg user=rancher . nerdctl nerdctl build --build-arg user=rancher . Using the Base Container to create the User Account We can utilize a multistage build to create the user in a container that provides the useradd binary and then copy all necessary files into SLE BCI Micro or SLE BCI Minimal. This is achieved using the following Dockerfile:\nFROM registry.suse.com/bci/bci-base:15.4 as useradder ARG user # omit -m if you don\u0026#39;t want /home/$user to be created RUN useradd -m $user FROM registry.suse.com/bci/bci-micro:15.4 ARG user COPY --from=useradder /etc/passwd /etc/passwd COPY --from=useradder /etc/group /etc/group COPY --from=useradder /etc/shadow /etc/shadow # subgid \u0026amp; subuid are rarely necessary in containers # COPY --from=useradder /etc/subgid /etc/subgid # COPY --from=useradder /etc/subuid /etc/subuid # some applications will send your user emails, in case yours does that, # uncomment the following line # COPY --from=useradder /var/spool/mail/$user /var/spool/mail/$user # only include this if you kept the -m flag to useradd COPY --from=useradder /home/$user /home/$user USER $user # remaining build / copy instructions go here Build your container image using your favorite container runtime using the --build-arg parameter as in Switch to using the BusyBox SLE BCI.\nUsing BusyBox to create the User Account We can leverage the adduser implementation from BusyBox to create new users in SLE BCI Minimal, by installing BusyBox inside the Minimal image and then executing its adduser. This will not work in the Micro image as it lacks rpm to install BusyBox.\nWarning This approach will leave two rpm files inside a layer of your final container image, thereby making it slightly bigger than necessary. Consider squashing the layers to remove this overhead. We utilize the SLE BCI Base container once again to download the rpms of BusyBox and libsepol1 (a dependency of BusyBox), copy both rpms into the Minimal image, add the user and remove both packages afterwards:\nFROM registry.suse.com/bci/bci-base:15.4 as downloader RUN zypper download busybox libsepol1 FROM registry.suse.com/bci/bci-minimal:15.4 ARG user ARG arch=x86_64 COPY --from=downloader /var/cache/zypp/packages/SLE_BCI/$arch/*rpm /tmp/ RUN rpm -i /tmp/libsepol1*rpm \u0026amp;\u0026amp; rpm -i /tmp/busybox*rpm \u0026amp;\u0026amp; \\ busybox adduser -D $user \u0026amp;\u0026amp; \\ rpm -e busybox \u0026amp;\u0026amp; rpm -e libsepol1 \u0026amp;\u0026amp; rm -rf /tmp/*rpm USER $user Building this container image requires the additional build argument arch when building on non-x86_64 systems. We also squash the layers, if supported by the container runtime. Currently nerdctl does not support squashing and Docker requires to be launched with experimental features enabled.\nDocker docker build --build-arg user=rancher \\ --build-arg arch=$(uname -m) \\ --squash . Podman buildah bud --build-arg user=rancher \\ --build-arg arch=$(uname -m) \\ --squash . nerdctl nerdctl build --build-arg user=rancher \\ --build-arg arch=$(uname -m) . ","description":" Note This guide will demonstrate how to add users to the SLE BCI Micro and SLE BCI Minimal images, without having the useradd binary installed. Background The SLE BCI Micro and Minimal images are tailored towards providing a small footprint and thus do not ship the useradd binary. While this reduces the image size, creating new users inside containers based on BCI Micro or Minimal involves a few additional steps.\n"},{"id":1,"href":"/bci-docs/documentation/general-purpose-bci/","title":"BCI-Base, BCI-Minimal, BCI-Micro, and BCI-BusyBox","parent":"Documentation","content":" SUSE offers several general-purpose SLE Base Container Images that are intended as deployment targets or as foundations for creating customized images: BCI-Base, BCI-Minimal, BCI-Micro, and BCI-BusyBox. These images share the common SLES base, and none of them ship with a specific language or an application stack. All images feature the RPM database (even if the specific image does not include the RPM package manager) that can be used to verify the provenance of every file in the image. Each image includes the SLES certificate bundle, which allows the deployed applications to use the system’s certificates to verify TLS connections.\nQuick overview The table below provides a quick overview of the differences between BCI-Base, BCI-Minimal, BCI-Micro, and BCI-BusyBox.\nBCI-Base and BCI-Init: When you need flexibility This SLE BCI comes with the Zypper package manager and a free SLE-BCI repository. This allows you to install software available in the repository and customize the image during the build. The downside is the size of the image. It is the largest of the general-purpose SLE BCIs, so it is not always the best choice for a deployment image.\nA variant of BCI-Base called BCI-Init comes with systemd preinstalled. The BCI-Init container image can be useful in scenarios requiring systemd for managing services in a single container.\nBCI-Minimal: When you do not need Zypper This is a stripped-down version of the BCI-Base image. BCI-Minimal comes without Zypper, but it does have the RPM package manager installed. This significantly reduces the size of the image. However, while RPM can install and remove packages, it lacks support for repositories and automated dependency resolution. The BCI-Minimal image is therefore intended for creating deployment containers, and then installing the desired RPM packages inside the containers. Although you can install the required dependencies, you need to download and resolve them manually. However, this approach is not recommended as it is prone to errors.\nBCI-Micro: When you need to deploy static binaries This image is similar to BCI-Minimal but without the RPM package manager. The primary use case for the image is deploying static binaries produced externally or during multi-stage builds. As there is no straightforward way to install additional dependencies inside the container image, we recommend deploying a project using the BCI-Micro image only when the final build artifact bundles all dependencies and has no external runtime requirements (like Python or Ruby).\nBCI-BusyBox: When you need the smallest and GPLv3-free image Similar to BCI-Micro, the BCI-BusyBox image comes with the most basic tools only. However, these tools are provided by the BusyBox project. This has the benefit of further size reduction. Furthermore, the image contains no GPLv3 licensed software. When using the image, keep in mind that there are certain differences between the BusyBox tools and the GNU Coreutils. So scripts written for a system that uses GNU Coreutils may require modification to work with BusyBox.\nApproximate sizes For your reference, the list below provides an approximate size of each SLE BCI. Keep in mind that the provided numbers are rough estimations.\nBCI-Base ~124 MB\nBCI-Minimal ~48 MB\nBCI-Micro ~26 MB\nBCI-BusyBox ~14 MB\n","description":" SUSE offers several general-purpose SLE Base Container Images that are intended as deployment targets or as foundations for creating customized images: BCI-Base, BCI-Minimal, BCI-Micro, and BCI-BusyBox. These images share the common SLES base, and none of them ship with a specific language or an application stack. All images feature the RPM database (even if the specific image does not include the RPM package manager) that can be used to verify the provenance of every file in the image. Each image includes the SLES certificate bundle, which allows the deployed applications to use the system’s certificates to verify TLS connections.\n"},{"id":2,"href":"/bci-docs/guides/use-with-golang/","title":"Building and Deploying Go Applications","parent":"Guides","content":" There is a SLE BCI that can be used with the Go programming language. There are a couple different recommended methods to work with the Go SLE BCI.\nDon’t Ship The Compiler Go is a compiled language producing a binary as the end result. That means the compiler does not need to be shipped as part of the images that are distributed. Instead, it is recommended that the Go image is used as the builder image only.\nBy not shipping the Go compiler with your application, the attack surface area of the containerized application is reduced and the overall image size is much smaller.\nUsing Go As A Builder Image There are two ways to work with Go images. First, you can encapsulate your application in a scratch container image, which is essentially an empty image. This approach will not function if your Go application depends on libc or any other library, as the libraries will not be available.\nA second method is to use a slim base container image with just the minimal packages needed. The General Purpose SLE BCI images offer four different options here, depending on your exact requirements.\nBuilding from scratch The following Dockerfile illustrates building an application using the SLE BCI Go image to compile the binary and then copying it into a new image based on scratch. The example uses a hello world as the program name, which can be substituted for the real application name.\n# Build the Go Binary using the SLE BCI Go 1.19 images FROM registry.suse.com/bci/golang:1.19 as build WORKDIR /app COPY go.mod ./ COPY go.sum ./ RUN go mod download COPY *.go ./ # Make sure to build the application with CGO disabled. This will force Go to # use some Go implementations of code rather than those normally supplied by the # host operating system. You need this for scratch images as those supporting # libraries are not available. RUN CGO_ENABLED=0 go build -o /hello-world # Create image to bundle app FROM scratch COPY --from=build /hello-world /hello-world CMD [\u0026#34;/hello-world\u0026#34;] Additional settings like exposing network ports or running as a non-root user can be specified in the last step below the FROM scratch line.\nBuilding from SLE BCI Applications that require external libraries or CA certificates cannot be deployed into a scratch image. A General Purpose SLE BCI should be used instead. The above Dockerfile has to be slightly adjusted in this case:\n# Build the Go Binary using the SLE BCI Go 1.19 images FROM registry.suse.com/bci/golang:1.19 as build WORKDIR /app COPY go.mod ./ COPY go.sum ./ RUN go mod download COPY *.go ./ RUN go build -o /hello-world # Create image to bundle app FROM registry.suse.com/bci/bci-micro:15.4 # Install dependencies (if required) here COPY --from=build /hello-world /usr/local/bin/hello-world CMD [\u0026#34;/usr/local/bin/hello-world\u0026#34;] The above example uses the SLE BCI micro image as the deployment image for the resulting application. This is just one of the options, other options can be found in the section about the General Purpose SLE BCI.\n","description":" There is a SLE BCI that can be used with the Go programming language. There are a couple different recommended methods to work with the Go SLE BCI.\nDon’t Ship The Compiler Go is a compiled language producing a binary as the end result. That means the compiler does not need to be shipped as part of the images that are distributed. Instead, it is recommended that the Go image is used as the builder image only.\n"},{"id":3,"href":"/bci-docs/guides/building-on-top-of-bci/","title":"Building Container Images Based on SLE BCI","parent":"Guides","content":" In the container ecosystem, many tools can work with OCI-compliant images, and all of them can use our Base Container Images.\nUsing with Docker, Podman or nerdctl The Base Container Images are OCI-compliant, and you can use them directly in your Dockerfile or Containerfile without any modifications. All you need to do is include the image in the FROM line as follows:\nFROM registry.suse.com/bci/nodejs:latest as node-builder WORKDIR /app/ COPY . /app/ RUN npm install \u0026amp;\u0026amp; npm run build Build it with your favorite container runtime: Docker docker build -t my-app . Podman podman build -t my-app . nerdctl nerdctl build -t my-app . Using the Open Build Service The Open Build Service (OBS) lets you build container images, as explained in the documentation. It supports Docker, Podman and KIWI as back-ends.\nThe examples below require a project where you have write access and osc. Your home project (home:\u0026lt;your-username\u0026gt;) is sufficient for this exercise.\nBuilding a container using Docker or Podman To build a Dockerfile-based container image in OBS, follow the steps below.\nCreate a new package for your container.\nosc checkout home:your-username \u0026amp;\u0026amp; cd home:your-username osc mkpac my-container-image \u0026amp;\u0026amp; cd my-container-image Add a repository to the project with osc. The repository name is containerfile by convention. This allows OBS to fetch resources used to build containers.\nosc meta -e prj Insert the following XML into the project settings:\n\u0026lt;repository name=\u0026#34;containerfile\u0026#34;\u0026gt; \u0026lt;path project=\u0026#34;SUSE:Registry\u0026#34; repository=\u0026#34;standard\u0026#34;/\u0026gt; \u0026lt;path project=\u0026#34;SUSE:SLE-15-SP7:Update\u0026#34; repository=\u0026#34;standard\u0026#34;/\u0026gt; \u0026lt;arch\u0026gt;x86_64\u0026lt;/arch\u0026gt; \u0026lt;arch\u0026gt;aarch64\u0026lt;/arch\u0026gt; \u0026lt;arch\u0026gt;s390x\u0026lt;/arch\u0026gt; \u0026lt;arch\u0026gt;ppc64le\u0026lt;/arch\u0026gt; \u0026lt;/repository\u0026gt; The path SUSE:Registry allows the use of images from the SUSE Registry in OBS. The path SUSE:SLE-15-SP7:Update allows the container to use packages from SLE. Depending on the SLE version that you are targeting in your container, this path needs adjustment.\nAdd a configuration to the project with osc. This will configure OBS to use Docker or Podman to build packages in the containerfile repository.\nosc meta -e prjconf Insert the following into the project configuration:\n%if %_repository == \u0026#34;containerfile\u0026#34; # OBS supports the following engines as backend: # - podman # - docker Type: podman %endif Create a Dockerfile inside the package my-container-image. Set the base image using FROM, as usual.\nFROM registry.suse.com/bci/bci-base:15.7 Set the build tags using comments in the Dockerfile.\n#!BuildTag: my-build-tag:latest #!BuildTag: my-build-tag:0.1 #!BuildTag: my-other-build-tag:0.1 The BuildTag is the equivalent image name (or tag) assigned during the build process. Since OBS invokes Docker or Podman itself, it has the same effect as the following command:\npodman build -t my-build-tag:latest -t my-build-tag:0.1 -t my-other-build-tag:0.1 . A complete Dockerfile would look like this:\n#!BuildTag: my-app:latest #!BuildTag: my-app:0.0.1 FROM registry.suse.com/bci/bci-base:15.7 WORKDIR /src RUN zypper -n install --no-recommends make gcc COPY . . RUN make CMD [\u0026#34;./my-app\u0026#34;] Building a container using KIWI KIWI is a generic image-building tool that also supports building container images. It is tightly integrated into the Open Build Service as the standard image builder.\nTo build a KIWI-based container image in OBS, follow the steps below.\nCreate a new package for your container.\nosc checkout home:your-username \u0026amp;\u0026amp; cd home:your-username osc mkpac my-kiwi-container \u0026amp;\u0026amp; cd my-kiwi-container Add a repository to the project with osc. The repository name is containerkiwi by convention. This allows OBS to fetch resources used to build containers.\nosc meta -e prj Insert the following XML into the project settings:\n\u0026lt;repository name=\u0026#34;containerkiwi\u0026#34;\u0026gt; \u0026lt;path project=\u0026#34;SUSE:Registry\u0026#34; repository=\u0026#34;standard\u0026#34;/\u0026gt; \u0026lt;path project=\u0026#34;SUSE:SLE-15-SP7:Update\u0026#34; repository=\u0026#34;standard\u0026#34;/\u0026gt; \u0026lt;arch\u0026gt;x86_64\u0026lt;/arch\u0026gt; \u0026lt;arch\u0026gt;aarch64\u0026lt;/arch\u0026gt; \u0026lt;arch\u0026gt;s390x\u0026lt;/arch\u0026gt; \u0026lt;arch\u0026gt;ppc64le\u0026lt;/arch\u0026gt; \u0026lt;/repository\u0026gt; Add a configuration to the project with osc. This will configure OBS to KIWI to build packages in the containerkiwi repository.\nosc meta -e prjconf Insert the following into the project configuration:\n%if \u0026#34;%_repository\u0026#34; == \u0026#34;containerkiwi\u0026#34; Type: kiwi Repotype: none Patterntype: none Prefer: -libcurl4-mini Prefer: -systemd-mini Prefer: -libsystemd0-mini Prefer: -libudev-mini1 Prefer: -udev-mini Prefer: kiwi-boot-requires Prefer: sles-release Prefer: sles-release-MINI Prefer: python3-kiwi Preinstall: !rpm rpm-ndb Substitute: rpm rpm-ndb Binarytype: rpm %endif Create a kiwi.xml inside the package my-kiwi-image.\n\u0026lt;image schemaversion=\u0026#34;7.4\u0026#34; name=\u0026#34;my-kiwi-image\u0026#34;\u0026gt; \u0026lt;description type=\u0026#34;system\u0026#34;\u0026gt; \u0026lt;!-- omitted --\u0026gt; \u0026lt;/description\u0026gt; \u0026lt;preferences\u0026gt; \u0026lt;!-- Refer to SLE BCI images by using `obsrepositories` --\u0026gt; \u0026lt;type image=\u0026#34;docker\u0026#34; derived_from=\u0026#34;obsrepositories:/bci/bci-base#15.7\u0026#34;\u0026gt; \u0026lt;containerconfig name=\u0026#34;my-kiwi-image\u0026#34; tag=\u0026#34;0.0.1\u0026#34; additionaltags=\u0026#34;latest\u0026#34;\u0026gt; \u0026lt;labels\u0026gt; \u0026lt;!-- add your labels here --\u0026gt; \u0026lt;label name=\u0026#34;org.opencontainers.image.title\u0026#34; value=\u0026#34;My KIWI Image\u0026#34;/\u0026gt; \u0026lt;/labels\u0026gt; \u0026lt;subcommand execute=\u0026#34;/bin/sh\u0026#34;/\u0026gt; \u0026lt;/containerconfig\u0026gt; \u0026lt;/type\u0026gt; \u0026lt;version\u0026gt;15.7.0\u0026lt;/version\u0026gt; \u0026lt;packagemanager\u0026gt;zypper\u0026lt;/packagemanager\u0026gt; \u0026lt;rpm-excludedocs\u0026gt;true\u0026lt;/rpm-excludedocs\u0026gt; \u0026lt;/preferences\u0026gt; \u0026lt;repository type=\u0026#34;rpm-md\u0026#34;\u0026gt; \u0026lt;source path=\u0026#34;obsrepositories:/\u0026#34;/\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;packages\u0026gt; \u0026lt;!-- add your packages here --\u0026gt; \u0026lt;package name=\u0026#34;gcc\u0026#34;/\u0026gt; \u0026lt;package name=\u0026#34;make\u0026#34;/\u0026gt; \u0026lt;/packages\u0026gt; \u0026lt;/image\u0026gt; Building images based on your images You can build containers in OBS that are based on other containers that have been built in OBS as well.\nIf the image you want to use is in the same project and repository as the image that you are building, there’s no need to configure any extra repositories.\nHowever, if the image comes from another project or repository, you need to adjust your repository configuration. Add the desired repository path to the project with osc. Previously, we added the repositories containerfile and containerkiwi to use the SLE BCI images. Now we are going to include another project path.\nosc meta -e prj Adjust the containerfile or containerkiwi XML to include a new path:\n\u0026lt;repository name=\u0026#34;containerfile\u0026#34;\u0026gt; \u0026lt;path project=\u0026#34;SUSE:Registry\u0026#34; repository=\u0026#34;standard\u0026#34;/\u0026gt; \u0026lt;path project=\u0026#34;SUSE:SLE-15-SP7:Update\u0026#34; repository=\u0026#34;standard\u0026#34;/\u0026gt; \u0026lt;path project=\u0026#34;PROJECT:NAME\u0026#34; repository=\u0026#34;repository-name\u0026#34;/\u0026gt; \u0026lt;arch\u0026gt;x86_64\u0026lt;/arch\u0026gt; \u0026lt;arch\u0026gt;aarch64\u0026lt;/arch\u0026gt; \u0026lt;arch\u0026gt;s390x\u0026lt;/arch\u0026gt; \u0026lt;arch\u0026gt;ppc64le\u0026lt;/arch\u0026gt; \u0026lt;/repository\u0026gt; As shown in the previous sections, now you can use other images similarly to SLE BCI.\n","description":" In the container ecosystem, many tools can work with OCI-compliant images, and all of them can use our Base Container Images.\nUsing with Docker, Podman or nerdctl The Base Container Images are OCI-compliant, and you can use them directly in your Dockerfile or Containerfile without any modifications. All you need to do is include the image in the FROM line as follows:\nFROM registry.suse.com/bci/nodejs:latest as node-builder WORKDIR /app/ COPY . /app/ RUN npm install \u0026amp;\u0026amp; npm run build "},{"id":4,"href":"/bci-docs/categories/","title":"Categories","parent":"Introduction to SLE Base Container Images","content":"","description":""},{"id":5,"href":"/bci-docs/guides/deploy-using-zypper/","title":"Deploy an Application using zypper","parent":"Guides","content":" Scope The purpose of this guide is to deploy an application or the dependencies of an application from rpms into a deployment image using the zypper package manager.\nUsing zypper’s custom root Zypper provides the --installroot flag to install packages into a custom root and not use /. We can leverage this to install packages including all of their dependencies into a custom root and then copy this directory into a deployment image. In the following example we install apache2 including all of its dependencies and copy them into the deployment image based on bci-micro:\nFROM registry.suse.com/bci/bci-micro:latest AS micro FROM registry.suse.com/bci/bci-base:latest AS builder COPY --from=micro / /chroot/ RUN zypper --installroot /chroot -n --gpg-auto-import-keys in --no-recommends apache2 \u0026amp;\u0026amp; \\ zypper --installroot /chroot clean -a \u0026amp;\u0026amp; \\ rm -rf /chroot/var/log/ FROM micro WORKDIR / COPY --from=builder /chroot/ / Customizing zypper’s installation behavior We can further reduce the final image size by supplying a custom zypper configuration file. We can tweak zypper’s behavior further via that configuration file to e.g. omit the installation of documentation files. To achieve this omission of documentation files, create the following scratch-zypp.conf:\n[main] rpm.install.excludedocs = yes And modify the Dockerfile as follows:\nFROM registry.suse.com/bci/bci-micro:latest AS micro FROM registry.suse.com/bci/bci-base:latest AS builder COPY --from=micro / /chroot/ COPY scratch-zypp.conf / ENV ZYPP_CONF=/scratch-zypp.conf RUN zypper --installroot /chroot -n --gpg-auto-import-keys in --no-recommends apache2 \u0026amp;\u0026amp; \\ zypper --installroot /chroot clean -a \u0026amp;\u0026amp; \\ rm -rf /chroot/var/log/ FROM micro WORKDIR / COPY --from=builder /chroot/ / ","description":" Scope The purpose of this guide is to deploy an application or the dependencies of an application from rpms into a deployment image using the zypper package manager.\nUsing zypper’s custom root Zypper provides the --installroot flag to install packages into a custom root and not use /. We can leverage this to install packages including all of their dependencies into a custom root and then copy this directory into a deployment image. In the following example we install apache2 including all of its dependencies and copy them into the deployment image based on bci-micro:\n"},{"id":6,"href":"/bci-docs/documentation/","title":"Documentation","parent":"Introduction to SLE Base Container Images","content":" Introduction Why SLE BCI General Purpose BCI Language Stack BCI Frequently Asked Questions ","description":" Introduction Why SLE BCI General Purpose BCI Language Stack BCI Frequently Asked Questions "},{"id":7,"href":"/bci-docs/documentation/faq/","title":"Frequently Asked Questions","parent":"Documentation","content":" What are the SUSE Linux Base Container Images? SUSE Linux Base Container Images (SUSE Linux BCI) provides a set of container images that are truly open, flexible and secure. Available for immediate use by developers, integrators and operators are container images based on SUSE Linux Enterprise Server (SLES). SUSE Linux BCI includes general-purpose container images, development tools and applications.\nWhat are the benefits of SUSE Linux BCI? Free to Use \u0026amp; Redistribute\nSUSE Linux BCI containers are completely free and come with an EULA that enables you to redistribute them freely.\nNo subscription required, ideal for development, CI/CD and production without subscription hurdles.\nEnterprise-Grade Security\nBuilt from the same code base as SUSE Linux Enterprise Server.\nMaintained with enterprise-grade CVE mitigation and aligned with SLES’s certified, proactive security.\nSLSA L3 grade builds.\nSigned container images for supply chain integrity.\nSBOM available in both SPDX and CycloneDX formats.\nSLSA Provenance file available.\nStability\nEnterprise lifecycle, aligned with the lifecycle of the corresponding SUSE Linux Enterprise Server version.\nConsistent ABI/API.\nVendor Neutral\nWorks on any OCI-compliant runtime.\nWorks on any Linux OS.\nNo vendor lock-in.\nMulti-architecture\nAvailable for up to 4 architectures[1]: amd64, arm64, ppc64le and s390x.\nCompliance \u0026amp; Certification\nEvaluation Assurance Level 4+ (Common Criteria) inherited from SUSE Linux Enterprise Server. SLES 15 is the only general-purpose OS with an active EAL 4+ certificate. See SUSE Certifications and Features for more details.\nSelected containers can be configured in FIPS mode.\nFIPS certification[2] inherited from SUSE Linux Enterprise Server for selected containers.\nSmooth transition to Enterprise Support\nStart free and when you need commercial support, subscribe and stay on the same container images.\nNo need to rebuild your stack when moving from community to enterprise.\nIs SUSE Linux BCI free to use? Yes. SUSE Linux BCI is free to use and redistribute in accordance with the SUSE Linux BCI EULA.\nWhere can I find SUSE Linux BCI? The container images are available on the SUSE Container Registry.\nWhat is included in SUSE Linux BCI? Leveraging SUSE’s industry-leading build pipeline, SUSE Linux BCI provides a set of container images where all binaries are built and maintained by SUSE, drawing directly from the trusted SLES codebase.\nBase containers come in several flavors. Choose the BCI Micro image for a minimal footprint or the BCI Base image with full package-management tools. Expand the container images easily using thousands of packages from the free BCI repository.\nLanguage containers are available as a base environment for development and deployment. Use programming languages such as C++, Go, Java (OpenJDK), .NET[3], Node.js, PHP, Python, Ruby and Rust.\nApplication containers provide ready-to-use databases, tools and more. Use applications such as 389 Directory Server, Cosign, Git, Helm, MariaDB, Nginx, Performance Co-Pilot, PostgreSQL, RMT and Valkey.\nWhat is the difference between SUSE Linux BCI Base, Minimal, Micro and BusyBox? The SUSE Linux BCI Base container has Zypper and access to the free BCI repository. This allows you to install any packages from the available repositories.\nThe SUSE Linux BCI Minimal container has RPM but does not include Zypper. It is an image designed for deployment scenarios where you copy the final artifacts to it.\nThe SUSE Linux BCI Micro container does not include Zypper and RPM. It is similar to BCI Minimal but designed for the deployment of static binaries.\nThe SUSE Linux BCI BusyBox container has no GPLv3-licensed software, Zypper, or RPM. It is comparable to BCI Micro, but it replaces the GNU Coreutils with BusyBox tools.\nWhat is the free BCI repository? The free BCI repository is a repository with packages that are free to use and redistribute, subject to the terms of the SUSE Linux BCI EULA.\nAll container images with Zypper include the free BCI repositories pre-configured.\nWhat packages and libraries can I use to expand SUSE Linux BCI? The container images that include Zypper have access to the free BCI repository. This repository contains thousands of SUSE-supported packages. SUSE will support these packages when the container is running on a SUSE host with the appropriate subscription.\nYou are also free to configure repositories from openSUSE Leap or other compatible sources.\nWhat are some use cases for SUSE Linux BCI? SUSE Linux BCI provides a stable, secure and open ecosystem for developing and deploying applications. SUSE Linux BCI leverages the experience, stability and security of SUSE Linux Enterprise Server.\nSUSE Linux BCI is suitable for different use cases:\nSUSE Rancher\nEnable Rancher to build using stable, reliable, secure and certified enterprise components.\nLeverage SUSE’s in-house OS knowledge while containerizing applications.\nEasy migration path from OS-based applications to containerized applications.\nDevelopers\nAvoid vendor lock-in.\nDeploy to any Linux host.\nFreely redistributable.\nSuitable for development, testing and production environments.\nIndependent Software Vendors (ISVs)\nContainerize applications using a stable, reliable, secure and certified enterprise OS.\nLeverage SUSE security and supply chain.\nRun applications on various hosts.\nWhy did SUSE create SUSE Linux BCI? SUSE created SUSE Linux BCI to provide truly open, flexible and secure container images that developers and integrators can use without the vendor lock-in of alternative offerings.\nOn which hardware platforms is SUSE Linux BCI available? SUSE Linux BCI is available on amd64 (x86_64), arm64 (aarch64), ppc64le and s390x. However, architecture availability may vary depending on the specific image.\nDo I need a subscription to use SUSE Linux BCI? No, you can use SUSE Linux BCI for free without a subscription. Just pull the images from the SUSE Container Registry and start using them.\nSUSE Linux BCI under an active SUSE Linux Enterprise Server subscription provides full support and access to SLES repositories.\nDo I need a SUSE host to build BCI-based images? No, you can build and run SUSE Linux BCI in any Linux environment that supports OCI-compatible images.\nDo I need a SUSE host to deploy SUSE Linux BCI? No, you can deploy SUSE Linux BCI to any OCI-compatible runtime or certified Kubernetes deployment.\nCan I distribute BCI-based images? Yes, BCI-based images can be freely distributed as long as you follow the SUSE Linux BCI EULA.\nAll packages in the free BCI repository are freely redistributable. If you use only this repository, your container remains redistributable.\nIf you add more content to your container, you may need to check for additional restrictions.\nCan I distribute BCI-based images on any registry? Yes, you can distribute your BCI-based image in any form or registry you want.\nCan I add third-party software to BCI-based images and still redistribute them? Yes, but adding third-party software or repositories implies restrictions at their respective layers.\nSUSE places no restrictions on redistributing the results, as long as you follow the SUSE Linux BCI EULA.\nDoes SUSE Linux BCI receive regular package updates? Yes, container images receive frequent security updates and version upgrades. It follows the same principles and release model as SUSE Linux Enterprise Server.\nDoes SUSE Linux BCI receive regular security updates? Yes, container images are rebuilt regularly and updated with SUSE’s enterprise-grade security updates.\nDoes SUSE Linux BCI have FIPS mode images? Yes, FIPS mode container images are available.\nBCI Base FIPS configures FIPS mode enabled by default. However, it does not include any certified binaries.\nBCI Go includes a FIPS 140-3 enabled Go.\nCan I use SUSE Linux BCI for community projects? Yes, as long as you follow the SUSE Linux BCI EULA.\nWhat is the lifecycle of SUSE Linux BCI? The SLES-based container images follow the same support lifecycle as their SLES release.\nApplication and Language images follow the lifecycle of their respective tools rather than the SLES release. For further details, check the SUSE Product Support Lifecycle.\nLong Term Service Pack (LTSS) versions of SUSE Linux BCI are available to customers with an LTSS subscription.\nHow is SUSE Linux BCI supported? As a free-to-use product under the SUSE Linux BCI EULA, SUSE Linux BCI receives community support.\nWhen BCI-based containers run on a SUSE host with an active subscription, they receive the same support as the host. This also applies to any packages installed from the free BCI repository.\nYou can install any package from the SLES repositories covered by your subscription and receive the same support. However, packages from the SLES repositories are not redistributable.\nWhere do I report bugs with SUSE Linux BCI? Customers and partners with active subscriptions can use the regular channels to report issues and request support.\nFor community support, report bugs in Bugzilla under PUBLIC SUSE Linux Base Container Images.\nWhere do I report issues with SUSE Linux BCI documentation? Report issues with documentation, guides and examples in Bugzilla under PUBLIC SUSE Linux Base Container Images.\nCan I request packages and images to be added to SUSE Linux BCI? Yes, you can request new features, but these require evaluation.\nCustomers and partners with active subscriptions can use the regular channels to request new features.\nFor community support, report bugs in Bugzilla under PUBLIC SUSE Linux Base Container Images.\n1. Some containers may be available in a smaller number of architectures. 2. Containers out of the General Availability phase are available to customers with corresponding LTS SUSE Linux Enterprise Server subscriptions. 3. Available in Tech Preview, .NET binaries provided and maintained by Microsoft. ","description":" What are the SUSE Linux Base Container Images? SUSE Linux Base Container Images (SUSE Linux BCI) provides a set of container images that are truly open, flexible and secure. Available for immediate use by developers, integrators and operators are container images based on SUSE Linux Enterprise Server (SLES). SUSE Linux BCI includes general-purpose container images, development tools and applications.\nWhat are the benefits of SUSE Linux BCI? Free to Use \u0026amp; Redistribute\nSUSE Linux BCI containers are completely free and come with an EULA that enables you to redistribute them freely.\nNo subscription required, ideal for development, CI/CD and production without subscription hurdles.\n"},{"id":8,"href":"/bci-docs/guides/","title":"Guides","parent":"Introduction to SLE Base Container Images","content":" Using SLE BCI Using with VS Code Development Containers Verify SLE BCI Building Container Images Based on SLE BCI Adding Users to BCI Micro and Minimal Building and Deploying Go Applications How to Use container-suseconnect Launch Containers with Podman and Systemd Deploy an Application Using zypper How to Build a Distroless Image Using SLE BCI ","description":" Using SLE BCI Using with VS Code Development Containers Verify SLE BCI Building Container Images Based on SLE BCI Adding Users to BCI Micro and Minimal Building and Deploying Go Applications How to Use container-suseconnect Launch Containers with Podman and Systemd Deploy an Application Using zypper How to Build a Distroless Image Using SLE BCI "},{"id":9,"href":"/bci-docs/guides/building-a-distroless-image/","title":"How to build a distroless image using SLE BCI","parent":"Guides","content":" What is a distroless image? Distroless images are stripped down container images, where the underlying Linux distribution is reduced to the bare minimum. A distroless image normally contains only certificates and specific core libraries, and it does not include a shell or utilities like cat or ls.\nThe advantages of distroless images include smaller size and potentially fewer vulnerabilities.\nThe major disadvantage is the difficulty of debugging a containerized application, as the container image does not provide any debugging tools and may even lack tools to read log files. Debugging applications usually requires attaching sidecar containers or interacting with the container via the /proc/ filesystem. For this reason, we recommend to use SLE BCI Base, SLE BCI Micro or SLE BCI BusyBox as the deployment image, because they make debugging easier, and they come with a valid rpm database.\nWarning A distroless image, or any image without the rpm database, makes it harder for container security scanners to find known vulnerabilities in the container image. Keep in mind that if a scanner relies only on the rpm database, it cannot detect a vulnerable shared library. How to build a distroless images from SLE BCI Warning The approach described in this section is unsupported and can lead to broken image as only shared libraries will be copied and no other required files are copied.\nFor a safer method, refer to Deploy an Application using zypper.\nAlthough SUSE does not offer a distroless SLE BCI, you can build one by creating a multi-stage build and creating a final image based on SCRATCH. The following tutorial uses an existing application to demonstrate how to identify its dependent libraries and copy them into the final image.\nThe first step is to identify all the components required by the application. This can include configuration files, external binaries, and shared libraries. It is your task to determine which configuration files and binaries are required for the program to function correctly. For example, a Python application will require that the Python interpreter is present in the final image.\nAs minimum, compiled applications are usually dynamically linked to libc. It is possible to statically link against most libraries, but not against glibc (which is the libc implementation used by SLE). Therefore the compiled application requires at least the shared libc library. The required libraries can be identified using ldd. The following example deploys the Rust package manager cargo in an empty image.\nRun the ldd /usr/bin/cargo command to obtain all shared libraries against which cargo is linked:\n# ldd /usr/bin/cargo linux-vdso.so.1 (0x00007ffda3f42000) libz.so.1 =\u0026gt; /lib64/libz.so.1 (0x00007f3766c14000) libcurl.so.4 =\u0026gt; /usr/lib64/libcurl.so.4 (0x00007f3767c6a000) libssl.so.1.1 =\u0026gt; /usr/lib64/libssl.so.1.1 (0x00007f3767bcb000) libcrypto.so.1.1 =\u0026gt; /usr/lib64/libcrypto.so.1.1 (0x00007f37668d5000) libgcc_s.so.1 =\u0026gt; /lib64/libgcc_s.so.1 (0x00007f37666b6000) libpthread.so.0 =\u0026gt; /lib64/libpthread.so.0 (0x00007f3766493000) libm.so.6 =\u0026gt; /lib64/libm.so.6 (0x00007f3766148000) libdl.so.2 =\u0026gt; /lib64/libdl.so.2 (0x00007f3765f44000) libc.so.6 =\u0026gt; /lib64/libc.so.6 (0x00007f3765b4f000) /lib64/ld-linux-x86-64.so.2 (0x00007f3767ae5000) libnghttp2.so.14 =\u0026gt; /usr/lib64/libnghttp2.so.14 (0x00007f3765927000) libidn2.so.0 =\u0026gt; /usr/lib64/libidn2.so.0 (0x00007f376570a000) libssh.so.4 =\u0026gt; /usr/lib64/libssh.so.4 (0x00007f376549c000) libpsl.so.5 =\u0026gt; /usr/lib64/libpsl.so.5 (0x00007f376528a000) libgssapi_krb5.so.2 =\u0026gt; /usr/lib64/libgssapi_krb5.so.2 (0x00007f3765038000) libldap_r-2.4.so.2 =\u0026gt; /usr/lib64/libldap_r-2.4.so.2 (0x00007f3764de4000) liblber-2.4.so.2 =\u0026gt; /usr/lib64/liblber-2.4.so.2 (0x00007f3764bd5000) libzstd.so.1 =\u0026gt; /usr/lib64/libzstd.so.1 (0x00007f37648a5000) libbrotlidec.so.1 =\u0026gt; /usr/lib64/libbrotlidec.so.1 (0x00007f3764699000) libjitterentropy.so.3 =\u0026gt; /usr/lib64/libjitterentropy.so.3 (0x00007f3764492000) libunistring.so.2 =\u0026gt; /usr/lib64/libunistring.so.2 (0x00007f376410f000) libkrb5.so.3 =\u0026gt; /usr/lib64/libkrb5.so.3 (0x00007f3763e36000) libk5crypto.so.3 =\u0026gt; /usr/lib64/libk5crypto.so.3 (0x00007f3763c1e000) libcom_err.so.2 =\u0026gt; /lib64/libcom_err.so.2 (0x00007f3763a1a000) libkrb5support.so.0 =\u0026gt; /usr/lib64/libkrb5support.so.0 (0x00007f376380b000) libresolv.so.2 =\u0026gt; /lib64/libresolv.so.2 (0x00007f37635f3000) libsasl2.so.3 =\u0026gt; /usr/lib64/libsasl2.so.3 (0x00007f37633d6000) libbrotlicommon.so.1 =\u0026gt; /usr/lib64/libbrotlicommon.so.1 (0x00007f37631b5000) libkeyutils.so.1 =\u0026gt; /usr/lib64/libkeyutils.so.1 (0x00007f3762fb0000) libselinux.so.1 =\u0026gt; /lib64/libselinux.so.1 (0x00007f3762d87000) libpcre.so.1 =\u0026gt; /usr/lib64/libpcre.so.1 (0x00007f3762afe000) The output lists all shared libraries that must be copied into the final image, with one exception: linux-vdso.so.1. This shared library is not present on the file system, it is a virtual shared library exported by the kernel for improved performance. Consult the manpage of vdso via man vdso for more information about linux-vdso.so.1.\nThe remaining shared libraries are required. We will parse the output of ldd and copy them into a directory while maintaining their directory structure into the final distroless image.\nStart with copying the shared libraries that are linked by their name and not their full path as follows:\nfor lib in $(ldd /usr/bin/cargo | cut -d\u0026#34; \u0026#34; -f 3); do install -Dp $lib /l$lib done The code above copies all libraries, including their directory structure, into the directory /l/:\n# tree /l /l |-- lib64 | |-- libc.so.6 | |-- libcom_err.so.2 | |-- libdl.so.2 | |-- libgcc_s.so.1 | |-- libm.so.6 | |-- libpthread.so.0 | |-- libresolv.so.2 | |-- libselinux.so.1 | `-- libz.so.1 `-- usr `-- lib64 |-- libbrotlicommon.so.1 |-- libbrotlidec.so.1 |-- libcrypto.so.1.1 |-- libcurl.so.4 |-- libgssapi_krb5.so.2 |-- libidn2.so.0 |-- libjitterentropy.so.3 |-- libk5crypto.so.3 |-- libkeyutils.so.1 |-- libkrb5.so.3 |-- libkrb5support.so.0 |-- liblber-2.4.so.2 |-- libldap_r-2.4.so.2 |-- libnghttp2.so.14 |-- libpcre.so.1 |-- libpsl.so.5 |-- libsasl2.so.3 |-- libssh.so.4 |-- libssl.so.1.1 |-- libunistring.so.2 `-- libzstd.so.1 3 directories, 30 files We are still missing the shared libraries linked by their full path, in this case that is only /lib64/ld-linux-x86-64.so.2. We can copy them using the following snippet:\nfor lib in $(ldd /usr/bin/cargo | grep \u0026#34;^[[:space:]]*/\u0026#34; | cut -d\u0026#34; \u0026#34; -f 1); do install -Dp $lib /l$lib done This makes all necessary libraries available under /l/:\n# tree /l /l |-- lib64 | |-- ld-linux-x86-64.so.2 | |-- libc.so.6 | |-- libcom_err.so.2 | |-- libdl.so.2 | |-- libgcc_s.so.1 | |-- libm.so.6 | |-- libpthread.so.0 | |-- libresolv.so.2 | |-- libselinux.so.1 | `-- libz.so.1 `-- usr `-- lib64 |-- libbrotlicommon.so.1 |-- libbrotlidec.so.1 |-- libcrypto.so.1.1 |-- libcurl.so.4 |-- libgssapi_krb5.so.2 |-- libidn2.so.0 |-- libjitterentropy.so.3 |-- libk5crypto.so.3 |-- libkeyutils.so.1 |-- libkrb5.so.3 |-- libkrb5support.so.0 |-- liblber-2.4.so.2 |-- libldap_r-2.4.so.2 |-- libnghttp2.so.14 |-- libpcre.so.1 |-- libpsl.so.5 |-- libsasl2.so.3 |-- libssh.so.4 |-- libssl.so.1.1 |-- libunistring.so.2 `-- libzstd.so.1 3 directories, 31 files With the required information in place, you can create a Dockerfile. The following example uses the SLE BCI Base image as the base image, installs cargo and creates the directory tree shown above:\nFROM registry.suse.com/bci/bci-base:15.4 as builder RUN zypper -n in cargo RUN for lib in $(ldd /usr/bin/cargo | cut -d\u0026#34; \u0026#34; -f 3); do \\ install -Dp $lib /l$lib; \\ done RUN for lib in $(ldd /usr/bin/cargo | grep \u0026#34;^[[:space:]]*/\u0026#34; | cut -d\u0026#34; \u0026#34; -f 1); do \\ install -Dp $lib /l$lib; \\ done Next, copy cargo itself and the libraries under /l/ into an empty image based on SCRATCH. As this image only contains cargo, set both CMD and ENTRYPOINT to cargo, to prevent the container behaving unexpectedly when it is launched without parameters. The complete Dockerfile looks as follows:\nFROM registry.suse.com/bci/bci-base:15.4 as builder RUN zypper -n in cargo RUN for lib in $(ldd /usr/bin/cargo | cut -d\u0026#34; \u0026#34; -f 3); do \\ install -Dp $lib /l$lib; \\ done RUN for lib in $(ldd /usr/bin/cargo | grep \u0026#34;^[[:space:]]*/\u0026#34; | cut -d\u0026#34; \u0026#34; -f 1); do \\ install -Dp $lib /l$lib; \\ done FROM scratch COPY --from=builder /l/ / COPY --from=builder /usr/bin/cargo /usr/bin/cargo ENTRYPOINT [\u0026#34;/usr/bin/cargo\u0026#34;] Build the image with the preferred container runtime:\nDocker docker build -t cargo . Podman buildah bud --layers -t cargo . nerdctl nerdctl build -t cargo . This creates a fully containerized ready-to-use cargo container:\nDocker ❯ docker run --rm -it cargo help Rust\u0026#39;s package manager Usage: cargo [OPTIONS] [COMMAND] Options: -V, --version Print version info and exit --list List installed commands --explain \u0026lt;CODE\u0026gt; Run `rustc --explain CODE` -v, --verbose... Use verbose output (-vv very verbose/build.rs output) -q, --quiet Do not print cargo log messages --color \u0026lt;WHEN\u0026gt; Coloring: auto, always, never --frozen Require Cargo.lock and cache are up to date --locked Require Cargo.lock is up to date --offline Run without accessing the network --config \u0026lt;KEY=VALUE\u0026gt; Override a configuration value -Z \u0026lt;FLAG\u0026gt; Unstable (nightly-only) flags to Cargo, see \u0026#39;cargo -Z help\u0026#39; for details -h, --help Print help information Some common cargo commands are (see all commands with --list): build, b Compile the current package check, c Analyze the current package and report errors, but don\u0026#39;t build object files clean Remove the target directory doc, d Build this package\u0026#39;s and its dependencies\u0026#39; documentation new Create a new cargo package init Create a new cargo package in an existing directory add Add dependencies to a manifest file remove Remove dependencies from a manifest file run, r Run a binary or example of the local package test, t Run the tests bench Run the benchmarks update Update dependencies listed in Cargo.lock search Search registry for crates publish Package and upload this package to the registry install Install a Rust binary. Default location is $HOME/.cargo/bin uninstall Uninstall a Rust binary See \u0026#39;cargo help \u0026lt;command\u0026gt;\u0026#39; for more information on a specific command. Podman ❯ podman run --rm -it localhost/cargo help Rust\u0026#39;s package manager Usage: cargo [OPTIONS] [COMMAND] Options: -V, --version Print version info and exit --list List installed commands --explain \u0026lt;CODE\u0026gt; Run `rustc --explain CODE` -v, --verbose... Use verbose output (-vv very verbose/build.rs output) -q, --quiet Do not print cargo log messages --color \u0026lt;WHEN\u0026gt; Coloring: auto, always, never --frozen Require Cargo.lock and cache are up to date --locked Require Cargo.lock is up to date --offline Run without accessing the network --config \u0026lt;KEY=VALUE\u0026gt; Override a configuration value -Z \u0026lt;FLAG\u0026gt; Unstable (nightly-only) flags to Cargo, see \u0026#39;cargo -Z help\u0026#39; for details -h, --help Print help information Some common cargo commands are (see all commands with --list): build, b Compile the current package check, c Analyze the current package and report errors, but don\u0026#39;t build object files clean Remove the target directory doc, d Build this package\u0026#39;s and its dependencies\u0026#39; documentation new Create a new cargo package init Create a new cargo package in an existing directory add Add dependencies to a manifest file remove Remove dependencies from a manifest file run, r Run a binary or example of the local package test, t Run the tests bench Run the benchmarks update Update dependencies listed in Cargo.lock search Search registry for crates publish Package and upload this package to the registry install Install a Rust binary. Default location is $HOME/.cargo/bin uninstall Uninstall a Rust binary See \u0026#39;cargo help \u0026lt;command\u0026gt;\u0026#39; for more information on a specific command. nerdctl ❯ nerdctl run --rm -it cargo help Rust\u0026#39;s package manager Usage: cargo [OPTIONS] [COMMAND] Options: -V, --version Print version info and exit --list List installed commands --explain \u0026lt;CODE\u0026gt; Run `rustc --explain CODE` -v, --verbose... Use verbose output (-vv very verbose/build.rs output) -q, --quiet Do not print cargo log messages --color \u0026lt;WHEN\u0026gt; Coloring: auto, always, never --frozen Require Cargo.lock and cache are up to date --locked Require Cargo.lock is up to date --offline Run without accessing the network --config \u0026lt;KEY=VALUE\u0026gt; Override a configuration value -Z \u0026lt;FLAG\u0026gt; Unstable (nightly-only) flags to Cargo, see \u0026#39;cargo -Z help\u0026#39; for details -h, --help Print help information Some common cargo commands are (see all commands with --list): build, b Compile the current package check, c Analyze the current package and report errors, but don\u0026#39;t build object files clean Remove the target directory doc, d Build this package\u0026#39;s and its dependencies\u0026#39; documentation new Create a new cargo package init Create a new cargo package in an existing directory add Add dependencies to a manifest file remove Remove dependencies from a manifest file run, r Run a binary or example of the local package test, t Run the tests bench Run the benchmarks update Update dependencies listed in Cargo.lock search Search registry for crates publish Package and upload this package to the registry install Install a Rust binary. Default location is $HOME/.cargo/bin uninstall Uninstall a Rust binary See \u0026#39;cargo help \u0026lt;command\u0026gt;\u0026#39; for more information on a specific command. ","description":" What is a distroless image? Distroless images are stripped down container images, where the underlying Linux distribution is reduced to the bare minimum. A distroless image normally contains only certificates and specific core libraries, and it does not include a shell or utilities like cat or ls.\nThe advantages of distroless images include smaller size and potentially fewer vulnerabilities.\nThe major disadvantage is the difficulty of debugging a containerized application, as the container image does not provide any debugging tools and may even lack tools to read log files. Debugging applications usually requires attaching sidecar containers or interacting with the container via the /proc/ filesystem. For this reason, we recommend to use SLE BCI Base, SLE BCI Micro or SLE BCI BusyBox as the deployment image, because they make debugging easier, and they come with a valid rpm database.\n"},{"id":10,"href":"/bci-docs/guides/container-suseconnect/","title":"How to use container-suseconnect","parent":"Guides","content":" What is container-suseconnect? container-suseconnect is a plugin available in all Base Container Images that ship with Zypper. When the plugin detects the host’s SUSE Linux Enterprise Server registration credentials, it uses them to give the container access the SUSE Linux Enterprise repositories. This includes additional modules and previous package versions that are not part of the free SLE_BCI repository.\nHow to use container-suseconnect If you are running a registered SLES system with Docker, container-suseconnect automatically detects and uses the subscription, without requiring any action on your part.\nOn openSUSE systems with Docker, you must copy the files /etc/SUSEConnect and /etc/zypp/credentials.d/SCCcredentials from a registered SLES machine to your local machine. Note that the /etc/SUSEConnect file is required only if you are using RMT for managing your registration credentials.\nHow to use container-suseconnect on non-SLE hosts or with Podman, Buildah or with nerdctl You need a registered SLES system to use container-suseconnect on non-SLE hosts or with Podman, Buildah, or with nerdctl. This can be a physical machine, a virtual machine, or the bci-base container with SUSEConnect installed and registered.\nIf you don’t use RMT, copy /etc/zypp/credentials.d/SCCcredentials to the development machine. Otherwise, copy both the /etc/zypp/credentials.d/SCCcredentials and /etc/SUSEConnect files.\nYou can use the following command to obtain SCCcredentials (replace REGISTRATION_CODE with your SCC registration code)\nDocker docker run --rm registry.suse.com/suse/sle15:latest bash -c \\ \u0026#34;zypper -n in SUSEConnect; SUSEConnect --regcode REGISTRATION_CODE; \\ cat /etc/zypp/credentials.d/SCCcredentials\u0026#34; Podman podman run --rm registry.suse.com/suse/sle15:latest bash -c \\ \u0026#34;zypper -n in SUSEConnect; SUSEConnect --regcode REGISTRATION_CODE; \\ cat /etc/zypp/credentials.d/SCCcredentials\u0026#34; nerdctl nerdctl run --rm registry.suse.com/suse/sle15:latest bash -c \\ \u0026#34;zypper -n in SUSEConnect; SUSEConnect --regcode REGISTRATION_CODE; \\ cat /etc/zypp/credentials.d/SCCcredentials\u0026#34; If you are running a container based on a SLE BCI, mount SCCcredentials (and optionally /etc/SUSEConnect) in the correct destination. The following example shows how to mount SCCcredentials in the current working directory:\nDocker docker run -v /path/to/SCCcredentials:/etc/zypp/credentials.d/SCCcredentials \\ -it --pull=always registry.suse.com/bci/bci-base:latest Podman podman run -v /path/to/SCCcredentials:/etc/zypp/credentials.d/SCCcredentials \\ -it --pull=always registry.suse.com/bci/bci-base:latest nerdctl nerdctl run -v /path/to/SCCcredentials:/etc/zypp/credentials.d/SCCcredentials \\ -it --pull=always registry.suse.com/bci/bci-base:latest Do not copy the SCCcredentials and SUSEConnect files into the container image to avoid inadvertently adding them to the final image. Use secrets instead, as they are only available to a single layer and are not part of the built image. To do this, put a copy of SCCcredentials (and optionally SUSEConnect) somewhere on the file system and modify the RUN instructions that invoke Zypper as follows:\nFROM registry.suse.com/bci/bci-base:latest RUN --mount=type=secret,id=SUSEConnect \\ --mount=type=secret,id=SCCcredentials \\ zypper -n in fluxbox Docker and Buildah both support mounting secrets via the --secret flag as follows: Docker docker build --secret=id=SCCcredentials,src=/path/to/SCCcredentials \\ --secret=id=SUSEConnect,src=/path/to/SUSEConnect . Podman buildah bud --layers --secret=id=SCCcredentials,src=/path/to/SCCcredentials \\ --secret=id=SUSEConnect,src=/path/to/SUSEConnect . Adding modules into the container or container Image container-suseconnect allows you to automatically add SLE Modules into a container or container image. What modules are added is determined by the environment variable ADDITIONAL_MODULES that includes a comma-separated list of the module names. In a Dockerfile, this is done using the ENV directive as follows:\nFROM registry.suse.com/bci/bci-base:latest ENV ADDITIONAL_MODULES sle-module-desktop-applications,sle-module-development-tools RUN --mount=type=secret,id=SCCcredentials zypper -n in fluxbox \u0026amp;\u0026amp; zypper -n clean ","description":" What is container-suseconnect? container-suseconnect is a plugin available in all Base Container Images that ship with Zypper. When the plugin detects the host’s SUSE Linux Enterprise Server registration credentials, it uses them to give the container access the SUSE Linux Enterprise repositories. This includes additional modules and previous package versions that are not part of the free SLE_BCI repository.\nHow to use container-suseconnect If you are running a registered SLES system with Docker, container-suseconnect automatically detects and uses the subscription, without requiring any action on your part.\n"},{"id":11,"href":"/bci-docs/guides/vscode-dev-containers/","title":"How To Use SLE BCIs As VScode Development Containers","parent":"Guides","content":" VS Code with the source mounted in a Development Container. The open terminal is a console within the container. Visual Studio Code has a feature called Development Containers. This is part of the built-in functionality to work with remote containers. The Language Stack SLE BCIs make a great choice to use as a development environment.\nWhy Use The SLE BCI In Development Containers There are two reasons the BCI language stack containers are useful for development.\nFirst, you develop in the same environment as when you use a Language Stack SLE BCI to build or run your application. Being able to develop and build or run your application in the same environment setup enables you to discover quirks and issues that are related to the way your application works in the environment.\nSecond, when you have a team of people working on an application, all developers will have the same environment to work in. Whether they are on Windows, macOS, or Linux their will be the same.\nDocker Socket Required VS Code creates the development containers using the Docker Engine and it communicates with it over the Docker socket. That means you need a Docker socket available on your system.\nRancher Desktop is our recommended app for working with containers. It is available for Windows, macOS, and Linux. Alternatively, you can use another tool such as Docker Desktop or the Docker Engine itself if you are using Linux.\nNote VS Code mounts your code from the local system inside the container. This works best when the container runtime is on your local system as opposed to remote. While possible to do this with the container runtime on a remote system, this guide does not cover running the development container on a remote machine. Development Container Basics Development containers enable you to mount any folder inside a container where you can specify the environment. Debugging, extensions, and other features work with the code as it is mounted within the container rather than the location on the local file system.\nDevelopment Container Configuration The configuration for Development Containers is stored in a file named .devcontainer/devcontainer.json. When you open up a codebase with this configuration file present, VS Code will read the configuration and present you with an option to use a Development Container.\nThere are two ways to specify where to get the container from. You can point it at an image or you can point it at a Dockerfile to build the image from. Since VS Code needs some additional packages installed you can’t simply point it at a SLE BCI image.\nTo illustrate using the Dockerfile method we can look at a setup for the Go programming language. A Dockerfile placed in the .devcontainer directory would look like:\nFROM registry.suse.com/bci/golang:1.18 # Install tools needed by Visual Studio Code Remote Development Containers RUN zypper --non-interactive install -y tar git gzip VS Code needs git, gzip, and tar installed which are not present in the Go SLE BCI image by default.\nWhile this example is targeted at Go, it will work for other languages where there is a BCI language stack available.\nThis file needs to be referenced in the devcontainer/devcontainer.json file. For example, a devcontainer.json could look like the following:\n{ \u0026#34;name\u0026#34;: \u0026#34;Golang\u0026#34;, \u0026#34;build\u0026#34;: { \u0026#34;dockerfile\u0026#34;: \u0026#34;Dockerfile\u0026#34; } } If you open up a project with these files in them VS Code will prompt you to open them in a container as the image below illustrates.\nPop-up asking if you want to open the code within a Development Container. This example JSON configuration file is in its simplest form. You can learn more details about the additional configuration in the VS Code documentation for Development Containers.\n","description":" VS Code with the source mounted in a Development Container. The open terminal is a console within the container. Visual Studio Code has a feature called Development Containers. This is part of the built-in functionality to work with remote containers. The Language Stack SLE BCIs make a great choice to use as a development environment.\n"},{"id":12,"href":"/bci-docs/","title":"Introduction to SLE Base Container Images","parent":"","content":" SLE Base Container Image (SLE BCI) are minimal SUSE Linux Enterprise Server 15-based images that you can use to develop, deploy, and share applications. There are two types of SLE BCI:\nGeneral-purpose SLE BCI can be used for building custom container images and for deploying applications.\nLanguage stack SLE BCI provide environments for developing and deploying applications in specific programming languages.\nIn addition to that, we will provide Application Container Images based on SLE BCI featuring popular containerized applications like Nginx, PostgreSQL, MariaDB and RMT.\nHighlights SLE BCI are fully compatible with SUSE Linux Enterprise Server, but they do not require a subscription to run and distribute them.\nSLE BCI automatically run in FIPS-compatible mode when the host operating system is running in FIPS mode.\nEach SLE BCI includes the RPM database, which makes it possible to audit the contents of the container image. You can use the RPM database to determine the specific version of the RPM package any given file belongs to. This allows you to ensure that a container image is not susceptible to known and already fixed vulnerabilities.\nAll SLE BCI (except for those without zypper) come with the container-suseconnect service. This gives containers that run on a registered SLES host access to the full SLES repositories. container-suseconnect is invoked automatically when you run zypper for the first time, and the service adds the correct SLES repositories into the running container. On an unregistered SLES host or on a non-SLES host, the service does nothing.\nGeneral-purpose SLE BCI There are four general purpose SLE BCI, and each container image comes with a minimum set of packages to keep its size low. You can use a general purpose SLE BCI either as a starting point for building custom container images, or as a platform for deploying specific software. For more information about general purpose SLE BCI, see here.\nLanguage stack SLE BCI Language stack SLE BCI are built on top of the BCI-Base general-purpose SLE BCI. Each container image comes with the zypper stack and the free SLE_BCI repository. Additionally, each image includes most common tools for building and deploying applications in the specific language environment. This includes tools like a compiler or interpreter as well as the language specific package manager. For more information about language stack SLE BCI, see here.\nImportant note on status and lifecycle All container images, except for bci-base, are currently classified as tech preview, and SUSE doesn’t provide official support for them. This information is visible on the web on registry.suse.com. In addition to that, it is also indicated via the com.suse.supportlevel label whether a container image still has the tech preview status. You can use the skopeo and jq utilities to check the status of the desired SLE BCI as follows:\n❯ skopeo inspect docker://registry.suse.com/bci/bci-micro:15.4 | jq \u0026#39;.Labels[\u0026#34;com.suse.supportlevel\u0026#34;]\u0026#39; \u0026#34;techpreview\u0026#34; ❯ skopeo inspect docker://registry.suse.com/bci/bci-base:15.4 | jq \u0026#39;.Labels[\u0026#34;com.suse.supportlevel\u0026#34;]\u0026#39; \u0026#34;l3\u0026#34; In the example above, the com.suse.supportlevel label is set to techpreview in the bci-micro container image, indicating that the image still has the tech preview status. The bci-base container image on the other hand is fully l3 supported. Unlike like the general purpose SLE BCI, the language stack SLE BCI may not follow the lifecycle of the SLE distribution: they are supported as long as the respective language stack receives support. In other words, new versions of SLE BCI (indicated by the OCI tags) may be released during the lifecycle of a SLE Service Pack, while older versions may become unsupported. Refer to suse.com/lifecycle to find out whether the container in question is still under support.\nGetting started The SLE BCI are available as OCI-compatible container images directly from registry.suse.com and can be used like any other container image. For example, using one of the general purpose containers:\nDocker ❯ docker run --rm -it registry.suse.com/bci/bci-base:15.4 grep \u0026#39;^NAME\u0026#39; /etc/os-release NAME=\u0026#34;SLES\u0026#34; Podman ❯ podman run --rm -it registry.suse.com/bci/bci-base:15.4 grep \u0026#39;^NAME\u0026#39; /etc/os-release NAME=\u0026#34;SLES\u0026#34; nerdctl ❯ nerdctl run --rm -it registry.suse.com/bci/bci-base:15.4 grep \u0026#39;^NAME\u0026#39; /etc/os-release NAME=\u0026#34;SLES\u0026#34; Alternatively, you can use SLE BCI in a Dockerfile as follows:\nFROM registry.suse.com/bci/bci-base:15.4 RUN zypper -n in python3 \u0026amp;\u0026amp; \\ echo \u0026#34;Hello Green World!\u0026#34; \u0026gt; index.html ENTRYPOINT [\u0026#34;/usr/bin/python3\u0026#34;, \u0026#34;-m\u0026#34;, \u0026#34;http.server\u0026#34;] EXPOSE 8000 You can then build container images using your favorite container runtime:\nDocker ❯ docker build . Sending build context to Docker daemon 2.048kB Step 1/4 : FROM registry.suse.com/bci/bci-base:15.4 ---\u0026gt; e34487b4c4e1 Step 2/4 : RUN zypper -n in python3 \u0026amp;\u0026amp; echo \u0026#34;Hello Green World!\u0026#34; \u0026gt; index.html ---\u0026gt; Using cache ---\u0026gt; 9b527dfa45e8 Step 3/4 : ENTRYPOINT [\u0026#34;/usr/bin/python3\u0026#34;, \u0026#34;-m\u0026#34;, \u0026#34;http.server\u0026#34;] ---\u0026gt; Using cache ---\u0026gt; 953080e91e1e Step 4/4 : EXPOSE 8000 ---\u0026gt; Using cache ---\u0026gt; 48b33ec590a6 Successfully built 48b33ec590a6 ❯ docker run -p 8000:8000 --rm -d 48b33ec590a6 575ad7edf43e11c2c9474055f7f6b7a221078739fc8ce5765b0e34a0c899b46a ❯ curl localhost:8000 Hello Green World! Podman ❯ buildah bud --layers . STEP 1/4: FROM registry.suse.com/bci/bci-base:15.4 STEP 2/4: RUN zypper -n in python3 \u0026amp;\u0026amp; echo \u0026#34;Hello Green World!\u0026#34; \u0026gt; index.html --\u0026gt; Using cache 8541a01ef66f1e43f850d30d756628fe301ae0ffe09dd3918d7e64d6e1788a3a --\u0026gt; 8541a01ef66 STEP 3/4: ENTRYPOINT [\u0026#34;/usr/bin/python3\u0026#34;, \u0026#34;-m\u0026#34;, \u0026#34;http.server\u0026#34;] --\u0026gt; Using cache 61cccdaa38aab5a44b0ef24935f4aa671f3231b611e0fa45c32ce869da6f9461 --\u0026gt; 61cccdaa38a STEP 4/4: EXPOSE 8000 --\u0026gt; Using cache 3e93a763b2d0a56ffe70429ca05a110288a868b46b92f47c1609a1129d058383 --\u0026gt; 3e93a763b2d 3e93a763b2d0a56ffe70429ca05a110288a868b46b92f47c1609a1129d058383 ❯ podman run --rm -d -p 8000:8000 3e93a763b2d0a56ffe70429ca05a110288a868b46b92f47c1609a1129d058383 e6115cbd37cf94781597cb7b8ade500951e7f4206b13102bdd9e603279378e17 ❯ curl localhost:8000 Hello Green World! ","description":" SLE Base Container Image (SLE BCI) are minimal SUSE Linux Enterprise Server 15-based images that you can use to develop, deploy, and share applications. There are two types of SLE BCI:\nGeneral-purpose SLE BCI can be used for building custom container images and for deploying applications.\nLanguage stack SLE BCI provide environments for developing and deploying applications in specific programming languages.\nIn addition to that, we will provide Application Container Images based on SLE BCI featuring popular containerized applications like Nginx, PostgreSQL, MariaDB and RMT.\n"},{"id":13,"href":"/bci-docs/documentation/language-stack-bci/","title":"Language stack SLE Base Container Images","parent":"Documentation","content":" If you have a working knowledge of containers, you will not have any difficulties using SLE BCIs. However, there are certain features that set SLE BCIs apart from similar offerings, like images based on Debian or Alpine Linux. And understanding the specifics can help you to get the most out of SLE BCIs in the shortest time possible.\nLanguage stack SLE BCIs Language stack SLE BCI are built on top of BCI-Base. Below is an overview of the available language stack SLE BCIs.\nPython repository: registry.suse.com/bci/python\nShips with the python3 version from the tag and pip3, curl, git tools.\nNode repository: registry.suse.com/bci/node\nComes with nodejs version from the tag, npm and git. The yarn package manager can be installed with the npm install -g yarn command.\nOpenJDK repository: registry.suse.com/bci/openjdk\nShips with the OpenJDK runtime. Designed for deploying Java applications.\nOpenJDK (devel) repository: registry.suse.com/bci/openjdk-devel\nIncludes the development part of OpenJDK in addition to the OpenJDK runtime. Instead of Bash, the default entry point is the jshell shell.\nRuby repository: registry.suse.com/bci/ruby\nA standard development environment featuring ruby, gem and bundler as well as git and curl.\nGo repository: registry.suse.com/bci/golang\nShips with the go compiler version specified in the tag.\n.NET (runtime) repository: registry.suse.com/bci/dotnet-runtime\nIncludes the .NET runtime from Microsoft and the Microsoft .NET repository.\n.NET (aspnet) repository: registry.suse.com/bci/dotnet-aspnet\nShips with the ASP.NET runtime from Microsoft and the Microsoft .NET repository.\n.Net (SDK) repository: registry.suse.com/suse/dotnet-sdk\nComes with the .NET and ASP.NET SDK from Microsoft as well as the Microsoft .NET repository.\nPHP repository: registry.suse.com/bci/php:latest\nShips with support for installing additional PHP extensions.\nC/C++ repository: registry.suse.com/bci/gcc\nIncludes also the support for Fortran.\nRust repository: registry.suse.com/bci/rust\nShips with the Rust compiler and the cargo package manager.\n","description":" If you have a working knowledge of containers, you will not have any difficulties using SLE BCIs. However, there are certain features that set SLE BCIs apart from similar offerings, like images based on Debian or Alpine Linux. And understanding the specifics can help you to get the most out of SLE BCIs in the shortest time possible.\nLanguage stack SLE BCIs Language stack SLE BCI are built on top of BCI-Base. Below is an overview of the available language stack SLE BCIs.\n"},{"id":14,"href":"/bci-docs/guides/podman-generate-systemd/","title":"Launch Containers with Podman and Systemd","parent":"Guides","content":" Local Container Orchestration A container runtime makes it easy to launch an application distributed as a single container. But things get more complicated when you need to run applications consisting of multiple containers, or when it’s necessary to start the applications automatically on system boot and restart them after they crash. While container orchestration tools like Kubernetes are designed for that exact purpose, they are intended to be used for highly distributed and scalable systems with hundreds of nodes, and not for a single machine. systemd and Podman are much better suited for the single-machine scenario, as they do not add another layer complexity to your existing setup.\nOverview Starting with version 1.3.0, Podman supports creating systemd unit files with the podman generate systemd subcommand. The subcommand creates a systemd unit file, making it possible to control a container or pod via systemd. Using the unit file you can launch a container or pod on boot, automatically restart it if a failure occurs, and keep its logs in journald.\nCreating a new Systemd Unit File The following example uses a simple NGINX container:\n❯ podman run -d --name web -p 8080:80 docker.io/nginx c0148d8476418a2da938a711542c55efc09e4119909aea70e287465c6fb51618 Generating a systemd unit for the container can be done as follows:\n❯ podman generate systemd --name --new web # container-web.service # autogenerated by Podman 4.2.0 # Tue Sep 13 10:58:54 CEST 2022 [Unit] Description=Podman container-web.service Documentation=man:podman-generate-systemd(1) Wants=network-online.target After=network-online.target RequiresMountsFor=%t/containers [Service] Environment=PODMAN_SYSTEMD_UNIT=%n Restart=on-failure TimeoutStopSec=70 ExecStartPre=/bin/rm -f %t/%n.ctr-id ExecStart=/usr/bin/podman run \\ --cidfile=%t/%n.ctr-id \\ --cgroups=no-conmon \\ --rm \\ --sdnotify=conmon \\ --replace \\ -d \\ --name web \\ -p 8080:80 docker.io/nginx ExecStop=/usr/bin/podman stop --ignore --cidfile=%t/%n.ctr-id ExecStopPost=/usr/bin/podman rm -f --ignore --cidfile=%t/%n.ctr-id Type=notify NotifyAccess=all [Install] WantedBy=default.target Podman outputs a unit file to the console that can be put either into the user unit systemd directories (~/.config/systemd/user/ or /etc/systemd/user/) or into the system unit systemd directory (/etc/systemd/systtem) and control the container via systemd. The --new flag instructs Podman to recreate the container on a restart. This ensures that the systemd unit is self-contained, and it does not depend on external state. The --name flag allows you to assign a user-friendly name to the container: without it Podman uses container IDs instead of their names.\nTo control the container as a user unit, proceed as follows:\n❯ podman generate systemd --name --new --files web /home/user/container-web.service ❯ mv container-web.service ~/.config/systemd/user/ ❯ systemctl --user daemon-reload Now the container can be started via systemctl --user start container-web:\n❯ systemctl --user start container-web ❯ systemctl --user is-active container-web.service active Run the podman ps command to see the list of all running containers :\n❯ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES af92743971d2 docker.io/library/nginx:latest nginx -g daemon o... 15 minutes ago Up 15 minutes ago 0.0.0.0:8080-\u0026gt;80/tcp web One of the benefits of managing the container via systemd is the ability to automatically restart the container if it crashes. You can simulate a crash by sending SIGKILL to the main process in the container:\n❯ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4c89582fa9cb docker.io/library/nginx:latest nginx -g daemon o... About a minute ago Up About a minute ago 0.0.0.0:8080-\u0026gt;80/tcp web ❯ kill -9 $(podman inspect --format \u0026#34;{{.State.Pid}}\u0026#34; web) ❯ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 0b5be4493251 docker.io/library/nginx:latest nginx -g daemon o... 4 seconds ago Up 4 seconds ago 0.0.0.0:8080-\u0026gt;80/tcp web Note that the container is not restarted when it is stopped gracefully, e.g. via podman stop web. To always restart it, add the flag --restart-policy=always to podman generate systemd.\nUpdating container images Using the described approach means that the container image is never updated. You can solve the problem by adding the --pull=always flag to the ExecStart= entry in the unit file. But be aware that this increases the startup time of the container and updates the image on every restart. The latter also means that a container image update can make the container unavailable outside of a scheduled maintenance window due to a newly introduced bug.\nThe auto-update subcommand in Podman provides a possible solution. Add the label io.containers.autoupdate=registry to a container to make Podman pull a new version of the container image from the registry when running podman auto-update. This makes it possible to update all container images with a single command at a desired time, and without increasing the startup time of the systemd units.\nThe auto update feature can be enabled by adding the line --label \u0026#34;io.containers.autoupdate=registry\u0026#34; \\ to the ExecStart= entry of the container’s systemd unit file. For the NGINX example, modify ~/.config/systemd/user/container-web.service as follows:\nExecStart=/usr/bin/podman run \\ --cidfile=%t/%n.ctr-id \\ --cgroups=no-conmon \\ --rm \\ --sdnotify=conmon \\ --replace \\ -d \\ --name web \\ --label \u0026#34;io.containers.autoupdate=registry\u0026#34; \\ -p 8080:80 docker.io/nginx After reloading the daemons and restarting the container, perform a dry run of the update (it will most likely not report any updates):\n❯ podman auto-update --dry-run UNIT CONTAINER IMAGE POLICY UPDATED container-web.service 87d263489307 (web) docker.io/nginx registry false It is good practice to have external testing in place to make sure that image updates are generally safe to be deployed. If you are confident in the quality of our container image, you can let Podman automatically apply image updates periodically by enabling the podman-auto-update.timer:\n# just for the current user ❯ systemctl --user enable podman-auto-update.timer Created symlink /home/user/.config/systemd/user/timers.target.wants/podman-auto-update.timer → /usr/lib/systemd/user/podman-auto-update.timer. # or as root ❯ sudo systemctl enable podman-auto-update.timer Created symlink /etc/systemd/system/timers.target.wants/podman-auto-update.timer → /usr/lib/systemd/system/podman-auto-update.timer. Managing multiple containers Certain applications rely on more than one container to function, for example a web frontend, a backend server and a database. Docker compose is popular tool for deploying multi-container applications on a single machine. While Podman does not support the compose command natively, in most cases compose files can be ported to a Podman pod and multiple containers.\nThe following example deploys a Drupal and PostgreSQL container in a single pod and manages these via systemd units. First, create a new pod that exposes the Drupal web interface:\n❯ podman pod create -p 8080:80 --name drupal 736cab072c49e68ad368ba819e9117be13ef8fa048a2eb88736b5968b3a19a64 Once the pod has been created, launch the Drupal frontend and the PostgreSQL database inside it:\n❯ podman run -d --name drupal-frontend --pod drupal docker.io/drupal ffd2fbd6d445e63fb0c28abb8d25ced78f819211d3bce9d6174fe4912d89f0ca ❯ podman run -d --name drupal-pg --pod drupal \\ -e POSTGRES_DB=drupal \\ -e POSTGRES_USER=user \\ -e POSTGRES_PASSWORD=pass \\ docker.io/postgres:11 a4dc31b24000780d9ffd81a486d0d144c47c3adfbecf0f7effee24a00273fcde This results in three running containers: the Drupal web interface, the PostgreSQL database and the pod’s infrastructure container.\n❯ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2948fa1476c6 localhost/podman-pause:4.2.0-1660228937 2 minutes ago Up About a minute ago 0.0.0.0:8080-\u0026gt;80/tcp 736cab072c49-infra ffd2fbd6d445 docker.io/library/drupal:latest apache2-foregroun... About a minute ago Up About a minute ago 0.0.0.0:8080-\u0026gt;80/tcp drupal-frontend a4dc31b24000 docker.io/library/postgres:11 postgres 40 seconds ago Up 41 seconds ago 0.0.0.0:8080-\u0026gt;80/tcp drupal-pg Creating a systemd unit for the pod is done similar to a single container:\n❯ podman generate systemd --name --new --files drupal /home/user/pod-drupal.service /home/user/container-drupal-frontend.service /home/user/container-drupal-pg.service ❯ mv *service ~/.config/systemd/user/ ❯ systemctl daemon-reload --user Since Podman is aware of which containers belong to the drupal pod and how their systemd units are called, it can correctly add the dependencies to the pod’s unit file. This means that when you start or stop the pod, systemd ensures that all containers inside the pod are started or stopped automatically.\nTo check systemd’s dependency handling, first stop the drupal pod and verify that no containers are currently running on the host:\n❯ podman pod stop drupal 736cab072c49e68ad368ba819e9117be13ef8fa048a2eb88736b5968b3a19a64 ❯ podman pod rm drupal 736cab072c49e68ad368ba819e9117be13ef8fa048a2eb88736b5968b3a19a64 ❯ podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Start the drupal pod via systemctl start --user pod-drupal.service, and systemd launches the containers inside the pod:\n❯ systemctl start --user pod-drupal.service ❯ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d1589d3ac68b localhost/podman-pause:4.2.0-1660228937 5 seconds ago Up 5 seconds ago 0.0.0.0:8080-\u0026gt;80/tcp ca41b505bd13-infra a49bea53c20c docker.io/library/postgres:11 postgres 4 seconds ago Up 5 seconds ago 0.0.0.0:8080-\u0026gt;80/tcp drupal-pg dc9dca018dad docker.io/library/drupal:latest apache2-foregroun... 4 seconds ago Up 5 seconds ago 0.0.0.0:8080-\u0026gt;80/tcp drupal-frontend ","description":" Local Container Orchestration A container runtime makes it easy to launch an application distributed as a single container. But things get more complicated when you need to run applications consisting of multiple containers, or when it’s necessary to start the applications automatically on system boot and restart them after they crash. While container orchestration tools like Kubernetes are designed for that exact purpose, they are intended to be used for highly distributed and scalable systems with hundreds of nodes, and not for a single machine. systemd and Podman are much better suited for the single-machine scenario, as they do not add another layer complexity to your existing setup.\n"},{"id":15,"href":"/bci-docs/tags/","title":"Tags","parent":"Introduction to SLE Base Container Images","content":"","description":""},{"id":16,"href":"/bci-docs/guides/using-sle-bci/","title":"Using SLE BCI","parent":"Guides","content":" Package manager The default package manager in SUSE Linux Enterprise is Zypper. Similar to APT in Debian and APK in Alpine Linux, Zypper offers a command-line interface for all package management tasks. Below is brief overview of commonly used container-related Zypper commands.\nInstall packages zypper --non-interactive install $PACKAGE_NAME Add a repository zypper --non-interactive addrepo $REPOSITORY_URL; zypper --non-interactive refresh Update all packages zypper --non-interactive update Remove a package zypper --non-interactive remove --clean-deps $PACKAGE_NAME the --clean-deps flag ensures that no longer required dependencies are removed as well\nClean up temporary files zypper clean For more information on using Zypper, refer to the zypper documentation.\nAll the described commands use the --non-interactive flag to skip confirmations, since you cannot approve these manually during container builds. Keep in mind that you must use the flag with any command that modifies the system. Also note that --non-interactive is not a \u0026#34;yes to all\u0026#34; flag. Instead, --non-interactive confirms what is considered to be the intention of the user. For example, an installation command with the --non-interactive option fails if it needs to import new repository signing keys, as that is something that the user should verify themselves.\nCommon patterns Here are a few examples that can give you an idea how to accomplish certain tasks in SLE BCI compared to Debian.\nRemove orphaned packages Debian: apt-get autoremove -y\nSLE BCI: Not required as long as you remove installed packages using the zypper --non-interactive remove --clean-deps $PACKAGE_NAME\nObtain container’s architecture Debian: dpkgArch=\u0026#34;$(dpkg --print-architecture | awk -F- \u0026#39;{ print $NF }\u0026#39;)\u0026#34;\nSLE BCI: arch=\u0026#34;$(uname -p|sed \u0026#39;s/x86_64/amd64/\u0026#39;)\u0026#34;\nInstall packages required for compilation Debian: apt-get install -y build-essential\nSLE BCI: zypper -n in gcc gcc-c++ make\nVerify GnuPG signatures Debian: gpg --batch --verify $SIGNATURE_URL $FILE_TO_VERIFY\nSLE BCI: zypper -n in dirmngr; gpg --batch --verify $SIGNATURE_URL $FILE_TO_VERIFY; zypper -n remove --clean-deps dirmngr; zypper -n clean\nPackage naming conventions SUSE Linux Enterprise package naming conventions differ from Debian, Ubuntu, and Alpine, and they are closer to those of Red Hat Enterprise Linux. The main difference is that development packages of libraries (that is, packages containing headers and build description files) are named $PACKAGE-devel in SUSE Linux Enterprise, as opposed to $PACKAGE-dev as they are in Debian and Ubuntu. When in doubt, search for the package directly using the following command:\nDocker docker run --rm registry.suse.com/bci/bci-base:$OS_VERSION zypper search $PACKAGE_NAME Podman podman run --rm registry.suse.com/bci/bci-base:$OS_VERSION zypper search $PACKAGE_NAME nerdctl nerdctl run --rm registry.suse.com/bci/bci-base:$OS_VERSION zypper search $PACKAGE_NAME (replace OS_VERSION with the appropriate service version number, for example: 15.3 or 15.4).\nAdding GPG signing keys Adding external repositories to a container or container image normally requires importing the GPG key used for signing the packages. This can be done with the rpm --import $KEY_URL command. This adds the key to the RPM database, and all packages from the repository can be installed afterwards.\n","description":" Package manager The default package manager in SUSE Linux Enterprise is Zypper. Similar to APT in Debian and APK in Alpine Linux, Zypper offers a command-line interface for all package management tasks. Below is brief overview of commonly used container-related Zypper commands.\nInstall packages zypper --non-interactive install $PACKAGE_NAME Add a repository zypper --non-interactive addrepo $REPOSITORY_URL; zypper --non-interactive refresh Update all packages zypper --non-interactive update Remove a package zypper --non-interactive remove --clean-deps $PACKAGE_NAME "},{"id":17,"href":"/bci-docs/guides/verify-sle-bci/","title":"Verify SLE Base Container Images","parent":"Guides","content":" Introduction Verifying container images allows you to confirm their provenance, thus ensuring the supply chain security. This document demonstrates how to verify container images using Cosign and how to integrate the verification step into your Podman installation.\nVerifying SLE BCI with Cosign To verify a SLE BCI image, run Cosign in the container. The command below fetches the signing key from the SUSE server and uses it to verify the latest BCI-Base container image.\n\u0026gt; podman run --rm -it gcr.io/projectsigstore/cosign verify \\ --key https://ftp.suse.com/pub/projects/security/keys/container-key.pem \\ registry.suse.com/bci/bci-base:latest | tail -1 | jq [ { \u0026#34;critical\u0026#34;: { \u0026#34;identity\u0026#34;: { \u0026#34;docker-reference\u0026#34;: \u0026#34;registry.suse.com/bci/bci-base\u0026#34; }, \u0026#34;image\u0026#34;: { \u0026#34;docker-manifest-digest\u0026#34;: \u0026#34;sha256:52a828600279746ef669cf02a599660cd53faf4b2430a6b211d593c3add047f5\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;cosign container image signature\u0026#34; }, \u0026#34;optional\u0026#34;: { \u0026#34;creator\u0026#34;: \u0026#34;OBS\u0026#34; } } ] The signing key can be used to verify all SLE BCI container images, and it also ships with SLE 15 (the /usr/share/container-keys/suse-container-key.pem file).\nYou can also check SLE BCI container images against rekor, the immutable tamper resistant ledger. For example:\n\u0026gt; podman run --rm -it -e COSIGN_EXPERIMENTAL=1 gcr.io/projectsigstore/cosign \\ verify --key https://ftp.suse.com/pub/projects/security/keys/container-key.pem \\ registry.suse.com/bci/bci-base:latest | tail -1 | jq [ { \u0026#34;critical\u0026#34;: { \u0026#34;identity\u0026#34;: { \u0026#34;docker-reference\u0026#34;: \u0026#34;registry.suse.com/bci/bci-base\u0026#34; }, \u0026#34;image\u0026#34;: { \u0026#34;docker-manifest-digest\u0026#34;: \u0026#34;sha256:52a828600279746ef669cf02a599660cd53faf4b2430a6b211d593c3add047f5\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;cosign container image signature\u0026#34; }, \u0026#34;optional\u0026#34;: { \u0026#34;creator\u0026#34;: \u0026#34;OBS\u0026#34; } } ] If verification fails, the output of the cosign verify command is similar to the one below.\nError: no matching signatures: crypto/rsa: verification error main.go:62: error during command execution: no matching signatures: crypto/rsa: verification error Verifying SLE BCI with Podman There are three main prerequisites to verify SLE Base Container Images using Podman. First, specify registry.suse.com as the registry for which image verification will be enabled.\nUpdate Podman registries configuration Note Skip this step on SLE or openSUSE, as the correct configuration is already in place. Add the following configuration to /etc/containers/registries.d/default.yaml:\ndocker: registry.suse.com: use-sigstore-attachments: true Instead of editing the default.yaml, you can create a new file in /etc/containers/registries.d/ with a filename of your choice.\nUpdate signature verification policy (/etc/containers/policy.json) Next, modify the /etc/containers/policy.json file. Under the docker attribute, add the registry.suse.com configuration similar to the following:\n{ \u0026#34;default\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;insecureAcceptAnything\u0026#34; } ], \u0026#34;transports\u0026#34;: { \u0026#34;docker-daemon\u0026#34;: { \u0026#34;\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;insecureAcceptAnything\u0026#34; } ] }, \u0026#34;docker\u0026#34;: { \u0026#34;registry.suse.com\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;sigstoreSigned\u0026#34;, \u0026#34;keyPath\u0026#34;: \u0026#34;/usr/share/pki/containers/suse-container-key.pem\u0026#34;, \u0026#34;signedIdentity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;matchRepository\u0026#34; } } ] } } } The specified configuration instructs Podman, Skopeo and Buildah to verify images under the registry.suse.com repository. This way, before pulling the image, Podman checks the validity of the signature using the specified public key, and rejects the image if the validation fails.\nNote Do not remove existing entries in transports.docker. Instead append the entry for registry.suse.com to the list. Fetch the SUSE Container signing key Note This step is optional on SLE. The signing key is already installed under /usr/share/pki/containers/suse-container-key.pem. Fetch the public key used to sign SLE BCIs from SUSE Signing Keys, or use the following command:\n\u0026gt; sudo curl -s https://ftp.suse.com/pub/projects/security/keys/container-key.pem \\ -o /usr/share/pki/containers/suse-container-key.pem Verifying if the image is signed Buildah, Podman and Skopeo will automatically verify every image pulled from registry.suse.com from now on. There are no additional steps required.\nIf verification fails, the command returns an error message as follows:\n\u0026gt; podman pull registry.suse.com/bci/bci-base:latest Trying to pull registry.suse.com/bci/bci-base:latest... Error: copying system image from manifest list: Source image rejected: Signature for identity registry.suse.com/bci/bci-base is not accepted If there are no issues with the signed image and your configuration, you can continue with your usual development workflow.\n","description":" Introduction Verifying container images allows you to confirm their provenance, thus ensuring the supply chain security. This document demonstrates how to verify container images using Cosign and how to integrate the verification step into your Podman installation.\nVerifying SLE BCI with Cosign To verify a SLE BCI image, run Cosign in the container. The command below fetches the signing key from the SUSE server and uses it to verify the latest BCI-Base container image.\n"},{"id":18,"href":"/bci-docs/documentation/why-sle-bci/","title":"Why SLE Base Container Images","parent":"Documentation","content":" SLE BCIs offer a platform for creating SLE-based custom container images and containerized applications that can be distributed freely. SLE BCIs feature the same predictable enterprise lifecycle as SLES. The SLE_BCI 15 SP3 and SP4 repository (which is a subset of the SLE repository) gives SLE BCIs access to 4,000 packages available for the AMD64/Intel 64, AArch64, ppc64le, and s390x architectures. The packages in the repository have undergone quality assurance and security audits by SUSE. The container images are FIPS-compliant when running on a host in FIPS mode. In addition to that, SUSE can provide official support for SLE BCIs through SUSE subscription plans.\nSecurity Each package in the SLE_BCI repository undergoes security audits, and SLE BCIs benefit from the same mechanism of dealing with CVEs as SLES. All discovered and fixed vulnerabilities are announced via e-mail, the dedicated CVE pages, and as OVAL and CVRF data. To ensure a secure supply chain, all container images are signed with Notary v1, Podman’s GPG signatures, and Sigstore Cosign.\nStability Since SLE BCIs are based on SLE, they feature the same level of stability and quality assurance as SUSE Linux Enterprise Server. Similar to SLES, SLE BCIs receive maintenance updates that provide bug fixes, improvements, and security patches.\nTooling and integration SLE BCIs are designed to provide drop-in replacements for popular container images available on hub.docker.com. You can use the general-purpose SLE BCIs and the tools they put at your disposal to create custom container images, while the language stack SLE BCIs provide a foundation and the required tooling for building containerized applications.\nRedistribution SLE BCIs are covered by a permissive EULA that allows you to redistribute custom container images based on a SLE BCI.\n","description":" SLE BCIs offer a platform for creating SLE-based custom container images and containerized applications that can be distributed freely. SLE BCIs feature the same predictable enterprise lifecycle as SLES. The SLE_BCI 15 SP3 and SP4 repository (which is a subset of the SLE repository) gives SLE BCIs access to 4,000 packages available for the AMD64/Intel 64, AArch64, ppc64le, and s390x architectures. The packages in the repository have undergone quality assurance and security audits by SUSE. The container images are FIPS-compliant when running on a host in FIPS mode. In addition to that, SUSE can provide official support for SLE BCIs through SUSE subscription plans.\n"}]